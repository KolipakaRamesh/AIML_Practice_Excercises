{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8, Day 3: Deep Q-Networks (DQN)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand DQN architecture\n",
    "- Learn experience replay\n",
    "- Master target networks\n",
    "- Practice implementing DQN\n",
    "\n",
    "## Topics Covered\n",
    "1. Neural Network Q-Learning\n",
    "2. Experience Replay\n",
    "3. Target Networks\n",
    "4. DQN Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        \n",
    "        targets = self.model.predict(states)\n",
    "        target_next = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i][actions[i]] = rewards[i] + self.gamma * np.amax(target_next[i])\n",
    "        \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_dqn():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQN(state_size, action_size)\n",
    "    batch_size = 32\n",
    "    episodes = 100\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                scores.append(time)\n",
    "                print(f\"Episode: {e+1}/{episodes}, Score: {time}, Epsilon: {agent.epsilon:.2}\")\n",
    "                break\n",
    "                \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = train_dqn()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(scores)\n",
    "plt.title('DQN Training Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQN Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DoubleDQN(DQN):\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        \n",
    "        targets = self.model.predict(states)\n",
    "        \n",
    "        # Double DQN: Use online network for action selection\n",
    "        next_actions = np.argmax(self.model.predict(next_states), axis=1)\n",
    "        # Use target network for value estimation\n",
    "        target_next = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i][actions[i]] = rewards[i] + self.gamma * target_next[i][next_actions[i]]\n",
    "        \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing DQN Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_dqn_variants():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Train DQN\n",
    "    dqn_agent = DQN(state_size, action_size)\n",
    "    dqn_scores = train_agent(env, dqn_agent, \"DQN\")\n",
    "    \n",
    "    # Train Double DQN\n",
    "    ddqn_agent = DoubleDQN(state_size, action_size)\n",
    "    ddqn_scores = train_agent(env, ddqn_agent, \"Double DQN\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(pd.Series(dqn_scores).rolling(10).mean(), label='DQN')\n",
    "    plt.plot(pd.Series(ddqn_scores).rolling(10).mean(), label='Double DQN')\n",
    "    plt.title('DQN vs Double DQN')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train_agent(env, agent, name, episodes=100):\n",
    "    scores = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        \n",
    "        for time in range(500):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                scores.append(time)\n",
    "                print(f\"{name} Episode: {e+1}/{episodes}, Score: {time}\")\n",
    "                break\n",
    "                \n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "compare_dqn_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Implement DQN\n",
    "\n",
    "def dqn_exercise():\n",
    "    print(\"Task: Implement basic DQN\")\n",
    "    print(\"1. Create neural network\")\n",
    "    print(\"2. Implement experience replay\")\n",
    "    print(\"3. Add target network\")\n",
    "    print(\"4. Train and evaluate\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "dqn_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Implement Double DQN\n",
    "\n",
    "def double_dqn_exercise():\n",
    "    print(\"Task: Implement Double DQN\")\n",
    "    print(\"1. Modify DQN architecture\")\n",
    "    print(\"2. Update target calculation\")\n",
    "    print(\"3. Train model\")\n",
    "    print(\"4. Compare with basic DQN\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "double_dqn_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is DQN?\n",
    "   - a) Policy gradient method\n",
    "   - b) Deep Q-learning\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "2. What is experience replay?\n",
    "   - a) Model architecture\n",
    "   - b) Memory mechanism\n",
    "   - c) Learning rate\n",
    "   - d) Activation function\n",
    "\n",
    "3. What is a target network?\n",
    "   - a) Main network\n",
    "   - b) Stable value network\n",
    "   - c) Policy network\n",
    "   - d) Feature extractor\n",
    "\n",
    "4. What is Double DQN?\n",
    "   - a) Two separate networks\n",
    "   - b) Action selection improvement\n",
    "   - c) Learning rate method\n",
    "   - d) Memory mechanism\n",
    "\n",
    "5. What is the epsilon-greedy strategy in DQN?\n",
    "   - a) Learning rate\n",
    "   - b) Exploration method\n",
    "   - c) Network architecture\n",
    "   - d) Loss function\n",
    "\n",
    "6. What is the purpose of target network?\n",
    "   - a) Faster learning\n",
    "   - b) Stable learning\n",
    "   - c) Better exploration\n",
    "   - d) Memory management\n",
    "\n",
    "7. What is the replay buffer?\n",
    "   - a) Neural network\n",
    "   - b) Experience storage\n",
    "   - c) Action selection\n",
    "   - d) Value function\n",
    "\n",
    "8. What is bootstrapping in DQN?\n",
    "   - a) Memory mechanism\n",
    "   - b) Value estimation\n",
    "   - c) Network architecture\n",
    "   - d) Action selection\n",
    "\n",
    "9. What is the advantage of Double DQN?\n",
    "   - a) Faster training\n",
    "   - b) Reduced overestimation\n",
    "   - c) Less memory usage\n",
    "   - d) Simpler architecture\n",
    "\n",
    "10. What is the role of batch size in DQN?\n",
    "    - a) Network architecture\n",
    "    - b) Training stability\n",
    "    - c) Action selection\n",
    "    - d) Memory size\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}