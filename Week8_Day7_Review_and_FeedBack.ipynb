{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8, Day 7: Review and Feedback Session\n",
    "\n",
    "## Session Overview\n",
    "This session will review the key concepts covered in Week 8 and provide practice exercises to reinforce learning:\n",
    "\n",
    "1. Basic RL Concepts\n",
    "2. Value-Based Methods\n",
    "3. Policy-Based Methods\n",
    "4. Advanced RL Topics\n",
    "\n",
    "## Learning Objectives\n",
    "- Reinforce RL concepts\n",
    "- Practice technique selection\n",
    "- Master implementation skills\n",
    "- Prepare for advanced topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic RL Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def basic_rl_review():\n",
    "    # Create simple environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Show environment info\n",
    "    print(\"Action Space:\", env.action_space)\n",
    "    print(\"State Space:\", env.observation_space)\n",
    "    \n",
    "    # Run one episode with random actions\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    print(\"\\nEpisode finished with reward:\", total_reward)\n",
    "\n",
    "basic_rl_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value-Based Methods Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def value_based_review():\n",
    "    # Q-Learning example\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    # Training parameters\n",
    "    alpha = 0.1\n",
    "    gamma = 0.95\n",
    "    epsilon = 0.1\n",
    "    episodes = 100\n",
    "    \n",
    "    # Training loop\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Q-Learning update\n",
    "            Q[state, action] = Q[state, action] + alpha * (\n",
    "                reward + gamma * np.max(Q[next_state]) - Q[state, action]\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pd.Series(rewards).rolling(10).mean())\n",
    "    plt.title('Q-Learning: Average Reward over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "\n",
    "value_based_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy-Based Methods Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_based_review():\n",
    "    # Simple policy gradient example\n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    # Build policy network\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(24, input_dim=env.observation_space.shape[0], activation='relu'),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(env.action_space.n, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Training loop\n",
    "    episodes = 100\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action probabilities\n",
    "            state_input = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
    "            action_probs = model(state_input)\n",
    "            action = np.random.choice(env.action_space.n, p=action_probs[0].numpy())\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(episode_rewards):\n",
    "            G = r + 0.99 * G\n",
    "            returns.insert(0, G)\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + 1e-8)\n",
    "        \n",
    "        # Update policy\n",
    "        with tf.GradientTape() as tape:\n",
    "            states = tf.convert_to_tensor(episode_states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(episode_actions, dtype=tf.int32)\n",
    "            action_probs = model(states)\n",
    "            \n",
    "            # Calculate loss\n",
    "            action_masks = tf.one_hot(actions, env.action_space.n)\n",
    "            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)\n",
    "            loss = -tf.reduce_mean(tf.math.log(selected_action_probs) * returns)\n",
    "        \n",
    "        # Apply gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        rewards_history.append(sum(episode_rewards))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pd.Series(rewards_history).rolling(10).mean())\n",
    "    plt.title('Policy Gradient: Average Reward over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "\n",
    "policy_based_review()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Review Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Choice Questions\n",
    "\n",
    "1. What is reinforcement learning?\n",
    "   - a) Supervised learning\n",
    "   - b) Learning from interaction\n",
    "   - c) Unsupervised learning\n",
    "   - d) Transfer learning\n",
    "\n",
    "2. What is Q-learning?\n",
    "   - a) Policy gradient method\n",
    "   - b) Value-based method\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "3. What is policy gradient?\n",
    "   - a) Value method\n",
    "   - b) Direct policy optimization\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "4. What is DQN?\n",
    "   - a) Policy method\n",
    "   - b) Deep Q-Network\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "5. What is SARSA?\n",
    "   - a) Off-policy method\n",
    "   - b) On-policy method\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "6. What is actor-critic?\n",
    "   - a) Value method\n",
    "   - b) Hybrid approach\n",
    "   - c) Model-based method\n",
    "   - d) Supervised learning\n",
    "\n",
    "7. What is exploration vs exploitation?\n",
    "   - a) Learning rate\n",
    "   - b) Action selection tradeoff\n",
    "   - c) Model architecture\n",
    "   - d) Loss function\n",
    "\n",
    "8. What is experience replay?\n",
    "   - a) Policy method\n",
    "   - b) Memory mechanism\n",
    "   - c) Model architecture\n",
    "   - d) Loss function\n",
    "\n",
    "9. What is multi-agent RL?\n",
    "   - a) Single agent\n",
    "   - b) Multiple agents\n",
    "   - c) Model-based\n",
    "   - d) Supervised\n",
    "\n",
    "10. What is hierarchical RL?\n",
    "    - a) Flat policy\n",
    "    - b) Nested policies\n",
    "    - c) Single policy\n",
    "    - d) Random policy\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 8 Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. Basic RL concepts and algorithms\n",
    "2. Value-based methods (Q-Learning, SARSA, DQN)\n",
    "3. Policy-based methods (Policy Gradients, Actor-Critic)\n",
    "4. Advanced topics (Multi-Agent, Hierarchical RL)\n",
    "\n",
    "### Preparation for Advanced Topics:\n",
    "- Review challenging concepts\n",
    "- Practice implementation\n",
    "- Study real-world applications\n",
    "- Explore latest research\n",
    "\n",
    "### Additional Resources:\n",
    "- OpenAI Spinning Up: https://spinningup.openai.com/\n",
    "- RL Course by David Silver: https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "- Stable Baselines3: https://stable-baselines3.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}