{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4, Day 3: Principal Component Analysis (PCA)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand dimensionality reduction concepts\n",
    "- Learn PCA algorithm and mathematics\n",
    "- Master variance explained analysis\n",
    "- Practice implementing PCA\n",
    "\n",
    "## Topics Covered\n",
    "1. Dimensionality Reduction\n",
    "2. PCA Algorithm\n",
    "3. Explained Variance\n",
    "4. Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_digits, load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic PCA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def basic_pca_example():\n",
    "    # Generate correlated data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    # Create features with correlation\n",
    "    x = np.random.normal(0, 1, n_samples)\n",
    "    y = x + np.random.normal(0, 0.3, n_samples)\n",
    "    X = np.column_stack([x, y])\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot original data and principal components\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Original data\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.5)\n",
    "    plt.title('Original Data')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    \n",
    "    # Plot principal components\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "    plt.title('Data in PC Space')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print explained variance ratios\n",
    "    print(\"Explained Variance Ratios:\")\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_, 1):\n",
    "        print(f\"PC{i}: {ratio:.4f}\")\n",
    "\n",
    "basic_pca_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def dimensionality_reduction_example():\n",
    "    # Load digits dataset\n",
    "    digits = load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot cumulative explained variance ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(range(1, len(cumsum) + 1), cumsum, 'bo-')\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize first two principal components\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('Digits Dataset in PC Space')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "    print(f\"Number of components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "dimensionality_reduction_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def feature_analysis_example():\n",
    "    # Load breast cancer dataset\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    feature_names = data.feature_names\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Get feature loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "        index=feature_names\n",
    "    )\n",
    "    \n",
    "    # Plot feature loadings heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(loadings.iloc[:, :5], annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Loadings (First 5 PCs)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "            pca.explained_variance_ratio_)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Scree Plot')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features for first 3 PCs\n",
    "    print(\"\\nTop features for first 3 principal components:\")\n",
    "    for i in range(3):\n",
    "        pc = loadings[f'PC{i+1}'].abs().sort_values(ascending=False)\n",
    "        print(f\"\\nPC{i+1} top features:\")\n",
    "        print(pc.head())\n",
    "\n",
    "feature_analysis_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Image Compression\n",
    "\n",
    "def image_compression_exercise():\n",
    "    from sklearn.datasets import load_sample_images\n",
    "    \n",
    "    # Load sample image\n",
    "    dataset = load_sample_images()\n",
    "    image = dataset.images[0]\n",
    "    \n",
    "    # Display original image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    print(\"Image shape:\", image.shape)\n",
    "    \n",
    "    # Task: Compress image using PCA\n",
    "    # 1. Prepare image data\n",
    "    # 2. Apply PCA\n",
    "    # 3. Reconstruct image\n",
    "    # 4. Compare original and compressed images\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "image_compression_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Stock Market Analysis\n",
    "\n",
    "def stock_analysis_exercise():\n",
    "    # Generate synthetic stock data\n",
    "    np.random.seed(42)\n",
    "    n_days = 1000\n",
    "    n_stocks = 10\n",
    "    \n",
    "    # Create correlated stock returns\n",
    "    market_factor = np.random.normal(0, 1, n_days)\n",
    "    stock_returns = np.zeros((n_days, n_stocks))\n",
    "    \n",
    "    for i in range(n_stocks):\n",
    "        beta = np.random.uniform(0.5, 1.5)\n",
    "        stock_returns[:, i] = beta * market_factor + np.random.normal(0, 0.5, n_days)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    stocks_df = pd.DataFrame(\n",
    "        stock_returns,\n",
    "        columns=[f'Stock_{i+1}' for i in range(n_stocks)]\n",
    "    )\n",
    "    \n",
    "    print(\"Sample of stock returns:\")\n",
    "    print(stocks_df.head())\n",
    "    \n",
    "    # Task: Analyze stock market factors using PCA\n",
    "    # 1. Prepare the data\n",
    "    # 2. Apply PCA\n",
    "    # 3. Analyze components\n",
    "    # 4. Visualize results\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "stock_analysis_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is the main purpose of PCA?\n",
    "   - a) Classification\n",
    "   - b) Dimensionality reduction\n",
    "   - c) Clustering\n",
    "   - d) Regression\n",
    "\n",
    "2. What does explained variance ratio represent?\n",
    "   - a) Error rate\n",
    "   - b) Proportion of variance captured\n",
    "   - c) Number of components\n",
    "   - d) Feature importance\n",
    "\n",
    "3. Which preprocessing step is crucial for PCA?\n",
    "   - a) Feature scaling\n",
    "   - b) Feature selection\n",
    "   - c) Outlier removal\n",
    "   - d) Missing value imputation\n",
    "\n",
    "4. What are principal components?\n",
    "   - a) Original features\n",
    "   - b) Linear combinations of features\n",
    "   - c) Random projections\n",
    "   - d) Cluster centers\n",
    "\n",
    "5. How are principal components ordered?\n",
    "   - a) Alphabetically\n",
    "   - b) By variance explained\n",
    "   - c) Randomly\n",
    "   - d) By correlation\n",
    "\n",
    "6. What is a scree plot used for?\n",
    "   - a) Feature selection\n",
    "   - b) Component selection\n",
    "   - c) Outlier detection\n",
    "   - d) Clustering\n",
    "\n",
    "7. What is the maximum number of principal components?\n",
    "   - a) Unlimited\n",
    "   - b) Number of features\n",
    "   - c) Number of samples\n",
    "   - d) Fixed number\n",
    "\n",
    "8. Which property is NOT true for principal components?\n",
    "   - a) Orthogonal\n",
    "   - b) Ordered by variance\n",
    "   - c) Correlated\n",
    "   - d) Linear combinations\n",
    "\n",
    "9. What is the time complexity of PCA?\n",
    "   - a) O(n)\n",
    "   - b) O(n log n)\n",
    "   - c) O(n²)\n",
    "   - d) O(n³)\n",
    "\n",
    "10. When should you NOT use PCA?\n",
    "    - a) High dimensional data\n",
    "    - b) Correlated features\n",
    "    - c) Categorical data\n",
    "    - d) Noisy data\n",
    "\n",
    "Answers: 1-b, 2-b, 3-a, 4-b, 5-b, 6-b, 7-b, 8-c, 9-d, 10-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}