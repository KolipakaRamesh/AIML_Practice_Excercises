{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8, Day 6: Reinforcement Learning Hackathon Challenge\n",
    "\n",
    "## Challenge Overview\n",
    "Build an end-to-end RL solution using concepts learned throughout Week 8:\n",
    "- Basic RL Concepts\n",
    "- Q-Learning and SARSA\n",
    "- Deep Q-Networks\n",
    "- Policy Gradients\n",
    "- Advanced RL Topics\n",
    "\n",
    "## Problem: Multi-Task Learning Agent\n",
    "Create a system that can learn and perform multiple tasks:\n",
    "1. Navigation and Pathfinding\n",
    "2. Resource Collection\n",
    "3. Strategic Decision Making\n",
    "4. Multi-Agent Coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiTaskEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskEnvironment, self).__init__()\n",
    "        \n",
    "        # Define action and observation space\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1,\n",
    "            shape=(10,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Initialize environment state\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.random(10)\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Execute action and get next state\n",
    "        next_state = self.state + np.random.normal(0, 0.1, 10)\n",
    "        next_state = np.clip(next_state, 0, 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward(action)\n",
    "        \n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = np.random.random() < 0.1\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def _calculate_reward(self, action):\n",
    "        # Implement reward calculation based on task and action\n",
    "        return np.random.random()\n",
    "\n",
    "# Create environment\n",
    "env = MultiTaskEnvironment()\n",
    "\n",
    "# Test environment\n",
    "state = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward:.2f}, Done: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiTaskAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # DQN for navigation\n",
    "        self.navigation_net = self._build_dqn()\n",
    "        \n",
    "        # Policy network for resource collection\n",
    "        self.resource_net = self._build_policy_net()\n",
    "        \n",
    "        # Actor-Critic for strategic decisions\n",
    "        self.actor = self._build_actor()\n",
    "        self.critic = self._build_critic()\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "    \n",
    "    def _build_dqn(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def _build_policy_net(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def _build_actor(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def _build_critic(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse',\n",
    "                     optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state, task):\n",
    "        if task == 'navigation':\n",
    "            return self._act_dqn(state)\n",
    "        elif task == 'resource':\n",
    "            return self._act_policy(state)\n",
    "        else:  # strategic\n",
    "            return self._act_actor(state)\n",
    "    \n",
    "    def _act_dqn(self, state):\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.navigation_net.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def _act_policy(self, state):\n",
    "        policy = self.resource_net.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, p=policy)\n",
    "    \n",
    "    def _act_actor(self, state):\n",
    "        policy = self.actor.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, p=policy)\n",
    "    \n",
    "    def train(self, task):\n",
    "        if len(self.memory) < 32:\n",
    "            return\n",
    "        \n",
    "        if task == 'navigation':\n",
    "            self._train_dqn()\n",
    "        elif task == 'resource':\n",
    "            self._train_policy()\n",
    "        else:  # strategic\n",
    "            self._train_actor_critic()\n",
    "    \n",
    "    def _train_dqn(self):\n",
    "        minibatch = random.sample(self.memory, 32)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(\n",
    "                    self.navigation_net.predict(next_state)[0]\n",
    "                )\n",
    "            target_f = self.navigation_net.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.navigation_net.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def _train_policy(self):\n",
    "        minibatch = random.sample(self.memory, 32)\n",
    "        states = np.array([x[0] for x in minibatch])\n",
    "        actions = np.array([x[1] for x in minibatch])\n",
    "        rewards = np.array([x[2] for x in minibatch])\n",
    "        \n",
    "        self.resource_net.fit(states, actions, sample_weight=rewards,\n",
    "                             epochs=1, verbose=0)\n",
    "    \n",
    "    def _train_actor_critic(self):\n",
    "        minibatch = random.sample(self.memory, 32)\n",
    "        states = np.array([x[0] for x in minibatch])\n",
    "        actions = np.array([x[1] for x in minibatch])\n",
    "        rewards = np.array([x[2] for x in minibatch])\n",
    "        next_states = np.array([x[3] for x in minibatch])\n",
    "        dones = np.array([x[4] for x in minibatch])\n",
    "        \n",
    "        # Train critic\n",
    "        values = self.critic.predict(states)\n",
    "        next_values = self.critic.predict(next_states)\n",
    "        targets = rewards + self.gamma * next_values * (1 - dones)\n",
    "        self.critic.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        # Train actor\n",
    "        advantages = targets - values\n",
    "        self.actor.fit(states, actions, sample_weight=advantages,\n",
    "                      epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_agent():\n",
    "    env = MultiTaskEnvironment()\n",
    "    agent = MultiTaskAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "    \n",
    "    tasks = ['navigation', 'resource', 'strategic']\n",
    "    episodes = 1000\n",
    "    scores = {task: [] for task in tasks}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Randomly select task\n",
    "        task = random.choice(tasks)\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            # Select and perform action\n",
    "            action = agent.act(state, task)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "            \n",
    "            # Store experience\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Train agent\n",
    "            agent.train(task)\n",
    "            \n",
    "            if done:\n",
    "                scores[task].append(total_reward)\n",
    "                print(f\"Episode: {episode + 1}, Task: {task}, Score: {total_reward}\")\n",
    "                break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Train agent\n",
    "scores = train_agent()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "for task in scores:\n",
    "    plt.plot(pd.Series(scores[task]).rolling(100).mean(), label=task)\n",
    "plt.title('Training Progress by Task')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Your solution will be evaluated based on:\n",
    "\n",
    "1. Navigation (25%)\n",
    "   - Path optimization\n",
    "   - Obstacle avoidance\n",
    "   - Learning efficiency\n",
    "\n",
    "2. Resource Collection (25%)\n",
    "   - Collection strategy\n",
    "   - Resource prioritization\n",
    "   - Efficiency metrics\n",
    "\n",
    "3. Strategic Decisions (25%)\n",
    "   - Decision quality\n",
    "   - Adaptation ability\n",
    "   - Long-term planning\n",
    "\n",
    "4. Multi-Agent Coordination (25%)\n",
    "   - Communication\n",
    "   - Cooperation\n",
    "   - Team performance\n",
    "\n",
    "## Submission Guidelines\n",
    "1. Complete all tasks in this notebook\n",
    "2. Document your approach and decisions\n",
    "3. Include visualizations and analysis\n",
    "4. Provide suggestions for improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}