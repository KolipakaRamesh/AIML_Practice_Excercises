{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6, Day 6: NLP Hackathon Challenge\n",
    "\n",
    "## Challenge Overview\n",
    "Build an end-to-end NLP solution using concepts learned throughout Week 6:\n",
    "- Text Processing\n",
    "- Classification\n",
    "- Named Entity Recognition\n",
    "- Topic Modeling\n",
    "- Language Models\n",
    "\n",
    "## Problem: Multi-Task NLP System\n",
    "Create a system that can perform multiple NLP tasks on news articles:\n",
    "1. Article Classification\n",
    "2. Entity Extraction\n",
    "3. Topic Analysis\n",
    "4. Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from gensim import corpora, models\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_sample_data(n_samples=100):\n",
    "    \"\"\"Generate synthetic news articles dataset\"\"\"\n",
    "    \n",
    "    # Categories\n",
    "    categories = ['Technology', 'Business', 'Sports', 'Politics']\n",
    "    \n",
    "    # Template sentences\n",
    "    tech_templates = [\n",
    "        \"Apple announced new {product} with advanced {feature}.\",\n",
    "        \"Google develops AI system for {application}.\",\n",
    "        \"Microsoft releases update for {software}.\"\n",
    "    ]\n",
    "    \n",
    "    business_templates = [\n",
    "        \"{company} reports {percent}% growth in Q3.\",\n",
    "        \"Stock market sees {direction} trend due to {factor}.\",\n",
    "        \"Merger announced between {company1} and {company2}.\"\n",
    "    ]\n",
    "    \n",
    "    sports_templates = [\n",
    "        \"{team} wins championship with score {score}.\",\n",
    "        \"Player {name} breaks record in {sport}.\",\n",
    "        \"Olympic committee announces {decision} for {event}.\"\n",
    "    ]\n",
    "    \n",
    "    politics_templates = [\n",
    "        \"President {name} announces new policy on {issue}.\",\n",
    "        \"Senate votes on {bill} legislation.\",\n",
    "        \"International summit discusses {topic}.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate articles\n",
    "    articles = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        category = np.random.choice(categories)\n",
    "        \n",
    "        if category == 'Technology':\n",
    "            template = np.random.choice(tech_templates)\n",
    "            article = template.format(\n",
    "                product=np.random.choice(['iPhone', 'MacBook', 'iPad']),\n",
    "                feature=np.random.choice(['AI', '5G', 'security'])\n",
    "            )\n",
    "        elif category == 'Business':\n",
    "            template = np.random.choice(business_templates)\n",
    "            article = template.format(\n",
    "                company=np.random.choice(['Amazon', 'Tesla', 'Netflix']),\n",
    "                percent=np.random.randint(5, 30),\n",
    "                direction=np.random.choice(['upward', 'downward']),\n",
    "                factor=np.random.choice(['inflation', 'growth', 'policy']),\n",
    "                company1=np.random.choice(['Company A', 'Company B']),\n",
    "                company2=np.random.choice(['Company C', 'Company D'])\n",
    "            )\n",
    "        elif category == 'Sports':\n",
    "            template = np.random.choice(sports_templates)\n",
    "            article = template.format(\n",
    "                team=np.random.choice(['Lakers', 'Warriors', 'Bulls']),\n",
    "                score=f\"{np.random.randint(80, 120)}-{np.random.randint(70, 110)}\",\n",
    "                name=np.random.choice(['John', 'Mike', 'Sarah']),\n",
    "                sport=np.random.choice(['basketball', 'football', 'tennis']),\n",
    "                decision=np.random.choice(['new rules', 'venue change']),\n",
    "                event=np.random.choice(['2024 Games', 'World Cup'])\n",
    "            )\n",
    "        else:  # Politics\n",
    "            template = np.random.choice(politics_templates)\n",
    "            article = template.format(\n",
    "                name=np.random.choice(['Smith', 'Johnson', 'Brown']),\n",
    "                issue=np.random.choice(['climate', 'economy', 'healthcare']),\n",
    "                bill=np.random.choice(['energy', 'tax', 'education']),\n",
    "                topic=np.random.choice(['trade', 'climate change', 'security'])\n",
    "            )\n",
    "        \n",
    "        articles.append(article)\n",
    "        labels.append(category)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'text': articles,\n",
    "        'category': labels\n",
    "    })\n",
    "\n",
    "# Generate dataset\n",
    "df = generate_sample_data()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nSample articles:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Article Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_classification():\n",
    "    \"\"\"Implement article classification\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Preprocess text\n",
    "    # 2. Create features\n",
    "    # 3. Train classifier\n",
    "    # 4. Evaluate performance\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_classification(df):\n",
    "    # Create features\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['category'])\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create and train model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(1000,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=10,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    return model, history, vectorizer, le\n",
    "\n",
    "model, history, vectorizer, le = example_classification(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_ner():\n",
    "    \"\"\"Implement named entity recognition\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Load NER model\n",
    "    # 2. Extract entities\n",
    "    # 3. Analyze results\n",
    "    # 4. Visualize entities\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_ner(df):\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    for text in df['text']:\n",
    "        doc = nlp(text)\n",
    "        doc_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        entities.append(doc_entities)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "entities = example_ner(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_topic_modeling():\n",
    "    \"\"\"Implement topic modeling\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Preprocess text\n",
    "    # 2. Create topic model\n",
    "    # 3. Extract topics\n",
    "    # 4. Visualize results\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_topic_modeling(df):\n",
    "    # Tokenize text\n",
    "    texts = [\n",
    "        word_tokenize(text.lower()) for text in df['text']\n",
    "    ]\n",
    "    \n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # Create corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = models.LdaModel(\n",
    "        corpus,\n",
    "        num_topics=4,\n",
    "        id2word=dictionary\n",
    "    )\n",
    "    \n",
    "    return lda_model, corpus, dictionary\n",
    "\n",
    "lda_model, corpus, dictionary = example_topic_modeling(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_summarization():\n",
    "    \"\"\"Implement text summarization\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Load summarization model\n",
    "    # 2. Generate summaries\n",
    "    # 3. Evaluate quality\n",
    "    # 4. Compare results\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_summarization(df):\n",
    "    # Initialize summarizer\n",
    "    summarizer = pipeline('summarization')\n",
    "    \n",
    "    # Generate summaries\n",
    "    summaries = []\n",
    "    for text in df['text']:\n",
    "        if len(text.split()) > 10:  # Check if text is long enough\n",
    "            summary = summarizer(text, max_length=30, min_length=10)[0]['summary_text']\n",
    "        else:\n",
    "            summary = text\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "summaries = example_summarization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Your solution will be evaluated based on:\n",
    "\n",
    "1. Classification (25%)\n",
    "   - Model accuracy\n",
    "   - Feature engineering\n",
    "   - Implementation quality\n",
    "\n",
    "2. Entity Extraction (25%)\n",
    "   - Entity coverage\n",
    "   - Accuracy\n",
    "   - Error analysis\n",
    "\n",
    "3. Topic Analysis (25%)\n",
    "   - Topic coherence\n",
    "   - Topic interpretability\n",
    "   - Visualization\n",
    "\n",
    "4. Summary Generation (25%)\n",
    "   - Summary quality\n",
    "   - Conciseness\n",
    "   - Information retention\n",
    "\n",
    "## Submission Guidelines\n",
    "1. Complete all tasks in this notebook\n",
    "2. Document your approach and decisions\n",
    "3. Include visualizations and analysis\n",
    "4. Provide suggestions for improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}