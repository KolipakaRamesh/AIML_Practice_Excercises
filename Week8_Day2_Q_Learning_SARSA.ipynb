{"cells":[{"cell_type":"markdown","metadata":{"id":"-gCsEtHPkDJU"},"source":["# Week 8, Day 2: Q-Learning and SARSA\n","\n","## Learning Objectives\n","- Understand Q-Learning algorithm\n","- Learn SARSA algorithm\n","- Master temporal difference learning\n","- Practice implementing value-based methods\n","\n","## Topics Covered\n","1. Q-Learning\n","2. SARSA\n","3. Exploration Strategies\n","4. Algorithm Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jR2ewtdxkDJW"},"source":["# Import required libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import gym\n","from collections import defaultdict\n","import random"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FlqRSeJYkDJW"},"source":["## 1. Q-Learning Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_9KAofYkDJW"},"source":["class QLearningAgent:\n","    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n","        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n","        self.lr = learning_rate\n","        self.gamma = discount_factor\n","        self.epsilon = epsilon\n","        self.n_actions = n_actions\n","\n","    def get_action(self, state):\n","        # Epsilon-greedy action selection\n","        if random.random() < self.epsilon:\n","            return random.randint(0, self.n_actions - 1)\n","        return np.argmax(self.q_table[state])\n","\n","    def learn(self, state, action, reward, next_state):\n","        # Q-Learning update\n","        best_next_action = np.argmax(self.q_table[next_state])\n","        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n","        td_error = td_target - self.q_table[state][action]\n","        self.q_table[state][action] += self.lr * td_error\n","\n","def train_q_learning():\n","    # Initialize environment\n","    env = gym.make('FrozenLake-v1')\n","    agent = QLearningAgent(env.observation_space.n, env.action_space.n)\n","\n","    # Training parameters\n","    episodes = 1000\n","    rewards_history = []\n","\n","    # Training loop\n","    for episode in range(episodes):\n","        state = env.reset()\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","\n","            agent.learn(state, action, reward, next_state)\n","            state = next_state\n","            total_reward += reward\n","\n","        rewards_history.append(total_reward)\n","\n","    # Plot results\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(pd.Series(rewards_history).rolling(100).mean())\n","    plt.title('Q-Learning: Average Reward over Episodes')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Average Reward')\n","    plt.show()\n","\n","    return agent\n","\n","q_learning_agent = train_q_learning()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3vEByJbkDJX"},"source":["## 2. SARSA Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrdiyVo1kDJX"},"source":["class SARSAAgent:\n","    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n","        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n","        self.lr = learning_rate\n","        self.gamma = discount_factor\n","        self.epsilon = epsilon\n","        self.n_actions = n_actions\n","\n","    def get_action(self, state):\n","        # Epsilon-greedy action selection\n","        if random.random() < self.epsilon:\n","            return random.randint(0, self.n_actions - 1)\n","        return np.argmax(self.q_table[state])\n","\n","    def learn(self, state, action, reward, next_state, next_action):\n","        # SARSA update\n","        td_target = reward + self.gamma * self.q_table[next_state][next_action]\n","        td_error = td_target - self.q_table[state][action]\n","        self.q_table[state][action] += self.lr * td_error\n","\n","def train_sarsa():\n","    # Initialize environment\n","    env = gym.make('FrozenLake-v1')\n","    agent = SARSAAgent(env.observation_space.n, env.action_space.n)\n","\n","    # Training parameters\n","    episodes = 1000\n","    rewards_history = []\n","\n","    # Training loop\n","    for episode in range(episodes):\n","        state = env.reset()\n","        action = agent.get_action(state)\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            next_state, reward, done, _ = env.step(action)\n","            next_action = agent.get_action(next_state)\n","\n","            agent.learn(state, action, reward, next_state, next_action)\n","\n","            state = next_state\n","            action = next_action\n","            total_reward += reward\n","\n","        rewards_history.append(total_reward)\n","\n","    # Plot results\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(pd.Series(rewards_history).rolling(100).mean())\n","    plt.title('SARSA: Average Reward over Episodes')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Average Reward')\n","    plt.show()\n","\n","    return agent\n","\n","sarsa_agent = train_sarsa()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZqkzIvikDJX"},"source":["## 3. Algorithm Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V12RoQOBkDJX"},"source":["def compare_algorithms():\n","    # Initialize environment\n","    env = gym.make('FrozenLake-v1')\n","\n","    # Training parameters\n","    episodes = 1000\n","    n_runs = 10\n","\n","    # Storage for results\n","    q_learning_rewards = np.zeros((n_runs, episodes))\n","    sarsa_rewards = np.zeros((n_runs, episodes))\n","\n","    # Run multiple training sessions\n","    for run in range(n_runs):\n","        # Q-Learning\n","        agent = QLearningAgent(env.observation_space.n, env.action_space.n)\n","        for episode in range(episodes):\n","            state = env.reset()\n","            total_reward = 0\n","            done = False\n","\n","            while not done:\n","                action = agent.get_action(state)\n","                next_state, reward, done, _ = env.step(action)\n","                agent.learn(state, action, reward, next_state)\n","                state = next_state\n","                total_reward += reward\n","\n","            q_learning_rewards[run, episode] = total_reward\n","\n","        # SARSA\n","        agent = SARSAAgent(env.observation_space.n, env.action_space.n)\n","        for episode in range(episodes):\n","            state = env.reset()\n","            action = agent.get_action(state)\n","            total_reward = 0\n","            done = False\n","\n","            while not done:\n","                next_state, reward, done, _ = env.step(action)\n","                next_action = agent.get_action(next_state)\n","                agent.learn(state, action, reward, next_state, next_action)\n","                state = next_state\n","                action = next_action\n","                total_reward += reward\n","\n","            sarsa_rewards[run, episode] = total_reward\n","\n","    # Plot comparison\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.plot(pd.Series(q_learning_rewards.mean(axis=0)).rolling(100).mean(),\n","             label='Q-Learning')\n","    plt.plot(pd.Series(sarsa_rewards.mean(axis=0)).rolling(100).mean(),\n","             label='SARSA')\n","\n","    plt.title('Q-Learning vs SARSA: Average Reward over Episodes')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Average Reward')\n","    plt.legend()\n","    plt.show()\n","\n","compare_algorithms()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyClG-cikDJY"},"source":["## Practical Exercises"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bo1SrIrhkDJY"},"source":["# Exercise 1: Q-Learning Implementation\n","\n","def q_learning_exercise():\n","    print(\"Task: Implement Q-Learning algorithm\")\n","    print(\"1. Initialize Q-table\")\n","    print(\"2. Implement epsilon-greedy\")\n","    print(\"3. Implement Q-update\")\n","    print(\"4. Train and evaluate\")\n","\n","    # Your code here\n","\n","q_learning_exercise()"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmg8k4iTkDJY"},"source":["# Exercise 2: SARSA Implementation\n","\n","def sarsa_exercise():\n","    print(\"Task: Implement SARSA algorithm\")\n","    print(\"1. Initialize Q-table\")\n","    print(\"2. Implement action selection\")\n","    print(\"3. Implement SARSA update\")\n","    print(\"4. Train and evaluate\")\n","\n","    # Your code here\n","\n","sarsa_exercise()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hj20ZmY9kDJY"},"source":["## MCQ Quiz\n","\n","1. What is Q-Learning?\n","   - a) Policy gradient method\n","   - b) Off-policy TD learning\n","   - c) Model-based method\n","   - d) Supervised learning\n","\n","2. What is SARSA?\n","   - a) Model-based method\n","   - b) On-policy TD learning\n","   - c) Supervised learning\n","   - d) Policy gradient\n","\n","3. What is temporal difference learning?\n","   - a) Supervised learning\n","   - b) Bootstrapping method\n","   - c) Policy gradient\n","   - d) Model-based learning\n","\n","4. What is epsilon-greedy?\n","   - a) Learning rate\n","   - b) Exploration strategy\n","   - c) Reward function\n","   - d) Value function\n","\n","5. What is off-policy learning?\n","   - a) Online learning\n","   - b) Different behavior policy\n","   - c) Model-based learning\n","   - d) Policy gradient\n","\n","6. What is the Q-value?\n","   - a) Reward\n","   - b) State-action value\n","   - c) Policy\n","   - d) Model\n","\n","7. What is the learning rate?\n","   - a) Exploration rate\n","   - b) Update step size\n","   - c) Discount factor\n","   - d) Reward scale\n","\n","8. What is bootstrapping?\n","   - a) Exploration\n","   - b) Using estimates\n","   - c) Policy update\n","   - d) Model learning\n","\n","9. What is the difference between Q-Learning and SARSA?\n","   - a) Learning rate\n","   - b) Policy difference\n","   - c) Model type\n","   - d) Reward scale\n","\n","10. What is the exploration-exploitation tradeoff?\n","    - a) Learning rate\n","    - b) Action selection balance\n","    - c) Policy type\n","    - d) Value function\n","\n","Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}