{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9, Day 4: Large Language Models\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand LLM architecture\n",
    "- Learn transformer models\n",
    "- Master attention mechanisms\n",
    "- Practice implementing LLMs\n",
    "\n",
    "## Topics Covered\n",
    "1. Transformer Architecture\n",
    "2. Attention Mechanisms\n",
    "3. Pre-training and Fine-tuning\n",
    "4. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = layers.Dense(d_model)\n",
    "        self.wk = layers.Dense(d_model)\n",
    "        self.wv = layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = layers.Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                     (batch_size, -1, self.d_model))\n",
    "        \n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, vocab_size, d_model=256, num_layers=4, num_heads=8):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        inputs = layers.Input(shape=(None,))\n",
    "        \n",
    "        # Embedding\n",
    "        x = layers.Embedding(self.vocab_size, self.d_model)(inputs)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = self.transformer_block(x)\n",
    "        \n",
    "        # Output\n",
    "        outputs = layers.Dense(self.vocab_size, activation='softmax')(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    def transformer_block(self, x):\n",
    "        # Multi-head attention\n",
    "        attn_output, _ = MultiHeadAttention(\n",
    "            self.d_model, self.num_heads)(x, x, x, None)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.point_wise_feed_forward_network(x)\n",
    "        return layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    def point_wise_feed_forward_network(self, x):\n",
    "        return layers.Dense(self.d_model * 4, activation='relu')(\n",
    "            layers.Dense(self.d_model)(x)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def use_pretrained_model():\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Generate text\n",
    "    def generate_text(prompt, max_length=100):\n",
    "        # Encode prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        \n",
    "        # Generate\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Decode and return\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Example usage\n",
    "    prompt = \"The artificial intelligence revolution\"\n",
    "    generated_text = generate_text(prompt)\n",
    "    print(f\"Generated text:\\n{generated_text}\")\n",
    "\n",
    "use_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prompt_engineering_examples():\n",
    "    # Load model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    # Different prompt styles\n",
    "    prompts = [\n",
    "        # Zero-shot\n",
    "        \"Classify the sentiment (positive/negative): 'I love this movie!'\",\n",
    "        \n",
    "        # Few-shot\n",
    "        \"\"\"Classify sentiment:\n",
    "        Text: 'This is terrible!' Sentiment: negative\n",
    "        Text: 'Amazing experience!' Sentiment: positive\n",
    "        Text: 'I'm so happy!' Sentiment:\"\"\",\n",
    "        \n",
    "        # Chain-of-thought\n",
    "        \"\"\"Let's solve this step by step:\n",
    "        Question: If John has 5 apples and gives 2 to Mary, how many does he have left?\n",
    "        1. Initial count: John has 5 apples\n",
    "        2. Given away: 2 apples to Mary\n",
    "        3. Calculation: 5 - 2 = 3\n",
    "        Therefore, John has\"\"\"\n",
    "    ]\n",
    "    \n",
    "    # Generate responses\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(prompt) + 50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(f\"\\nPrompt: {prompt}\\nResponse: {response}\\n\")\n",
    "\n",
    "prompt_engineering_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Attention Mechanism\n",
    "\n",
    "def attention_exercise():\n",
    "    print(\"Task: Implement attention mechanism\")\n",
    "    print(\"1. Create attention layer\")\n",
    "    print(\"2. Implement scaled dot-product\")\n",
    "    print(\"3. Add multi-head attention\")\n",
    "    print(\"4. Test attention\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "attention_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Prompt Engineering\n",
    "\n",
    "def prompt_exercise():\n",
    "    print(\"Task: Design effective prompts\")\n",
    "    print(\"1. Create zero-shot prompts\")\n",
    "    print(\"2. Design few-shot examples\")\n",
    "    print(\"3. Implement chain-of-thought\")\n",
    "    print(\"4. Test prompts\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "prompt_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is a transformer?\n",
    "   - a) RNN model\n",
    "   - b) Attention-based model\n",
    "   - c) CNN model\n",
    "   - d) Linear model\n",
    "\n",
    "2. What is attention?\n",
    "   - a) Memory mechanism\n",
    "   - b) Focus mechanism\n",
    "   - c) Learning rate\n",
    "   - d) Loss function\n",
    "\n",
    "3. What is self-attention?\n",
    "   - a) External attention\n",
    "   - b) Internal attention\n",
    "   - c) Memory mechanism\n",
    "   - d) Loss function\n",
    "\n",
    "4. What is prompt engineering?\n",
    "   - a) Model training\n",
    "   - b) Input design\n",
    "   - c) Loss function\n",
    "   - d) Architecture design\n",
    "\n",
    "5. What is zero-shot learning?\n",
    "   - a) Training method\n",
    "   - b) No example inference\n",
    "   - c) Loss function\n",
    "   - d) Architecture type\n",
    "\n",
    "6. What is few-shot learning?\n",
    "   - a) Full training\n",
    "   - b) Example-based learning\n",
    "   - c) Architecture type\n",
    "   - d) Loss function\n",
    "\n",
    "7. What is chain-of-thought?\n",
    "   - a) Model architecture\n",
    "   - b) Reasoning process\n",
    "   - c) Training method\n",
    "   - d) Loss function\n",
    "\n",
    "8. What is pre-training?\n",
    "   - a) Fine-tuning\n",
    "   - b) Initial training\n",
    "   - c) Inference\n",
    "   - d) Evaluation\n",
    "\n",
    "9. What is fine-tuning?\n",
    "   - a) Pre-training\n",
    "   - b) Task adaptation\n",
    "   - c) Model design\n",
    "   - d) Loss function\n",
    "\n",
    "10. What is temperature in generation?\n",
    "    - a) Model parameter\n",
    "    - b) Randomness control\n",
    "    - c) Learning rate\n",
    "    - d) Loss weight\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}