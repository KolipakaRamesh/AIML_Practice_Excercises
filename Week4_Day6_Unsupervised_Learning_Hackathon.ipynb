{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4, Day 6: Unsupervised Learning Hackathon\n",
    "\n",
    "## Challenge Overview\n",
    "Apply unsupervised learning techniques to analyze a real-world dataset. You'll use the concepts learned throughout Week 4:\n",
    "- Clustering\n",
    "- Dimensionality Reduction\n",
    "- Anomaly Detection\n",
    "- Visualization\n",
    "\n",
    "## Problem: E-commerce Customer Analysis\n",
    "Analyze customer behavior patterns and identify segments in an e-commerce dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_ecommerce_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic e-commerce customer data\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate features\n",
    "    data = {\n",
    "        'purchase_frequency': np.random.poisson(5, n_samples),\n",
    "        'avg_order_value': np.random.normal(100, 30, n_samples),\n",
    "        'time_on_site': np.random.gamma(5, 2, n_samples),\n",
    "        'items_viewed': np.random.poisson(20, n_samples),\n",
    "        'cart_abandonment_rate': np.random.beta(2, 5, n_samples),\n",
    "        'days_since_last_purchase': np.random.exponential(30, n_samples),\n",
    "        'total_purchases': np.random.poisson(15, n_samples),\n",
    "        'discount_usage': np.random.binomial(10, 0.3, n_samples),\n",
    "        'customer_service_calls': np.random.poisson(2, n_samples),\n",
    "        'website_visits': np.random.poisson(30, n_samples)\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some anomalies\n",
    "    n_anomalies = 50\n",
    "    anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
    "    \n",
    "    df.loc[anomaly_indices, 'avg_order_value'] *= 5\n",
    "    df.loc[anomaly_indices, 'website_visits'] *= 3\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate dataset\n",
    "df = generate_ecommerce_data()\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Tasks\n",
    "\n",
    "### Task 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def perform_eda(df):\n",
    "    \"\"\"Perform exploratory data analysis\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Analyze feature distributions\n",
    "    # 2. Check correlations\n",
    "    # 3. Identify patterns\n",
    "    # 4. Visualize relationships\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_eda(df):\n",
    "    # Basic statistics\n",
    "    print(\"Basic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Correlation analysis\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature distributions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        plt.subplot(3, 4, i)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "example_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def segment_customers(df):\n",
    "    \"\"\"Perform customer segmentation\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Preprocess data\n",
    "    # 2. Apply clustering\n",
    "    # 3. Analyze segments\n",
    "    # 4. Visualize results\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_segmentation(df):\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply K-means\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to DataFrame\n",
    "    df_clustered = df.copy()\n",
    "    df_clustered['Cluster'] = clusters\n",
    "    \n",
    "    # Analyze clusters\n",
    "    print(\"\\nCluster Sizes:\")\n",
    "    print(df_clustered['Cluster'].value_counts())\n",
    "    \n",
    "    # Cluster profiles\n",
    "    print(\"\\nCluster Profiles:\")\n",
    "    print(df_clustered.groupby('Cluster').mean())\n",
    "    \n",
    "    # Visualize clusters\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('Customer Segments (PCA)')\n",
    "    plt.show()\n",
    "\n",
    "example_segmentation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies(df):\n",
    "    \"\"\"Detect anomalous customers\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Preprocess data\n",
    "    # 2. Apply detection methods\n",
    "    # 3. Compare results\n",
    "    # 4. Visualize anomalies\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_anomaly_detection(df):\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    anomalies = iso_forest.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add anomaly labels\n",
    "    df_anomalies = df.copy()\n",
    "    df_anomalies['Anomaly'] = anomalies\n",
    "    \n",
    "    # Analyze anomalies\n",
    "    print(\"\\nNumber of anomalies detected:\", (anomalies == -1).sum())\n",
    "    print(\"\\nAnomaly characteristics:\")\n",
    "    print(df_anomalies.groupby('Anomaly').mean())\n",
    "    \n",
    "    # Visualize anomalies\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca[anomalies == 1, 0], X_pca[anomalies == 1, 1], \n",
    "                label='Normal')\n",
    "    plt.scatter(X_pca[anomalies == -1, 0], X_pca[anomalies == -1, 1], \n",
    "                color='red', label='Anomaly')\n",
    "    plt.title('Anomaly Detection Results')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "example_anomaly_detection(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_patterns(df):\n",
    "    \"\"\"Visualize customer patterns\"\"\"\n",
    "    # Your code here:\n",
    "    # 1. Apply dimensionality reduction\n",
    "    # 2. Create visualizations\n",
    "    # 3. Analyze patterns\n",
    "    # 4. Compare methods\n",
    "    pass\n",
    "\n",
    "# Example solution structure:\n",
    "def example_visualization(df):\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Apply different methods\n",
    "    pca = PCA(n_components=2)\n",
    "    umap = UMAP(random_state=42)\n",
    "    \n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_umap = umap.fit_transform(X_scaled)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # PCA\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "    plt.title('PCA Projection')\n",
    "    \n",
    "    # UMAP\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_umap[:, 0], X_umap[:, 1], alpha=0.5)\n",
    "    plt.title('UMAP Projection')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze explained variance (PCA)\n",
    "    print(\"\\nPCA Explained Variance Ratio:\")\n",
    "    print(pca.explained_variance_ratio_)\n",
    "\n",
    "example_visualization(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Your solution will be evaluated based on:\n",
    "\n",
    "1. Data Analysis (25%)\n",
    "   - Quality of EDA\n",
    "   - Pattern identification\n",
    "   - Insight generation\n",
    "\n",
    "2. Customer Segmentation (25%)\n",
    "   - Segment identification\n",
    "   - Segment analysis\n",
    "   - Business insights\n",
    "\n",
    "3. Anomaly Detection (25%)\n",
    "   - Method selection\n",
    "   - Implementation quality\n",
    "   - Result interpretation\n",
    "\n",
    "4. Visualization (25%)\n",
    "   - Clarity of visualizations\n",
    "   - Pattern representation\n",
    "   - Technical implementation\n",
    "\n",
    "## Submission Guidelines\n",
    "1. Complete all tasks in this notebook\n",
    "2. Document your approach and decisions\n",
    "3. Include visualizations and insights\n",
    "4. Provide business recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}