{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9, Day 2: Advanced GANs and VAEs\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand advanced GAN architectures\n",
    "- Learn conditional generation\n",
    "- Master advanced VAE techniques\n",
    "- Practice implementing advanced models\n",
    "\n",
    "## Topics Covered\n",
    "1. Conditional GANs\n",
    "2. CycleGAN\n",
    "3. Conditional VAEs\n",
    "4. Beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conditional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ConditionalGAN:\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build generator and discriminator\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        # Noise input\n",
    "        noise = layers.Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        # Label input\n",
    "        label = layers.Input(shape=(1,))\n",
    "        label_embedding = layers.Embedding(self.num_classes, 50)(label)\n",
    "        label_embedding = layers.Flatten()(label_embedding)\n",
    "        \n",
    "        # Combine noise and label\n",
    "        combined = layers.Concatenate()([noise, label_embedding])\n",
    "        \n",
    "        # Generator network\n",
    "        x = layers.Dense(7*7*256)(combined)\n",
    "        x = layers.Reshape((7, 7, 256))(x)\n",
    "        \n",
    "        x = layers.Conv2DTranspose(128, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        output = layers.Conv2D(1, 4, padding='same', activation='tanh')(x)\n",
    "        \n",
    "        return tf.keras.Model([noise, label], output)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        # Image input\n",
    "        image = layers.Input(shape=(28, 28, 1))\n",
    "        \n",
    "        # Label input\n",
    "        label = layers.Input(shape=(1,))\n",
    "        label_embedding = layers.Embedding(self.num_classes, 50)(label)\n",
    "        label_embedding = layers.Flatten()(label_embedding)\n",
    "        label_embedding = layers.Dense(28*28)(label_embedding)\n",
    "        label_embedding = layers.Reshape((28, 28, 1))(label_embedding)\n",
    "        \n",
    "        # Combine image and label\n",
    "        combined = layers.Concatenate()([image, label_embedding])\n",
    "        \n",
    "        # Discriminator network\n",
    "        x = layers.Conv2D(64, 4, strides=2, padding='same')(combined)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Flatten()(x)\n",
    "        output = layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        return tf.keras.Model([image, label], output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CycleGAN:\n",
    "    def __init__(self):\n",
    "        # Build generators and discriminators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        # U-Net Generator\n",
    "        inputs = layers.Input(shape=(256, 256, 3))\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.encoder_block(inputs, 64, batchnorm=False)\n",
    "        e2 = self.encoder_block(e1, 128)\n",
    "        e3 = self.encoder_block(e2, 256)\n",
    "        e4 = self.encoder_block(e3, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = self.decoder_block(e4, e3, 256)\n",
    "        d2 = self.decoder_block(d1, e2, 128)\n",
    "        d3 = self.decoder_block(d2, e1, 64)\n",
    "        \n",
    "        outputs = layers.Conv2DTranspose(3, 4, strides=2, padding='same',\n",
    "                                        activation='tanh')(d3)\n",
    "        \n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        # PatchGAN discriminator\n",
    "        inputs = layers.Input(shape=(256, 256, 3))\n",
    "        \n",
    "        x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        outputs = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def encoder_block(self, x, filters, batchnorm=True):\n",
    "        x = layers.Conv2D(filters, 4, strides=2, padding='same')(x)\n",
    "        if batchnorm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "    \n",
    "    def decoder_block(self, x, skip, filters):\n",
    "        x = layers.Conv2DTranspose(filters, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ConditionalVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2D(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim + latent_dim)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(latent_dim + num_classes,)),\n",
    "            layers.Dense(7*7*32, activation='relu'),\n",
    "            layers.Reshape((7, 7, 32)),\n",
    "            layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x, c):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = tf.split(x, num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        z_c = tf.concat([z, c], axis=1)\n",
    "        return self.decoder(z_c)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x, c = inputs\n",
    "        mean, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z, c)\n",
    "        return x_logit, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Beta-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class BetaVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, beta=4.0):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2D(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(latent_dim + latent_dim)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(latent_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(7*7*64, activation='relu'),\n",
    "            layers.Reshape((7, 7, 64)),\n",
    "            layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "        return x_logit, mean, logvar\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_div = -0.5 * (1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "        kl_div = tf.reduce_sum(kl_div, axis=1)\n",
    "        \n",
    "        # Beta-VAE loss\n",
    "        return -tf.reduce_mean(logpx_z - self.beta * kl_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Conditional Generation\n",
    "\n",
    "def conditional_generation_exercise():\n",
    "    print(\"Task: Implement conditional generation\")\n",
    "    print(\"1. Create conditional model\")\n",
    "    print(\"2. Process condition input\")\n",
    "    print(\"3. Train model\")\n",
    "    print(\"4. Generate samples\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "conditional_generation_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Style Transfer\n",
    "\n",
    "def style_transfer_exercise():\n",
    "    print(\"Task: Implement style transfer\")\n",
    "    print(\"1. Create transfer model\")\n",
    "    print(\"2. Process style and content\")\n",
    "    print(\"3. Train model\")\n",
    "    print(\"4. Generate transfers\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "style_transfer_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is conditional generation?\n",
    "   - a) Random generation\n",
    "   - b) Controlled generation\n",
    "   - c) Data processing\n",
    "   - d) Model training\n",
    "\n",
    "2. What is CycleGAN?\n",
    "   - a) Basic GAN\n",
    "   - b) Unpaired translation\n",
    "   - c) Classification model\n",
    "   - d) Regression model\n",
    "\n",
    "3. What is a conditional VAE?\n",
    "   - a) Basic VAE\n",
    "   - b) Conditional generation\n",
    "   - c) Classification model\n",
    "   - d) Regression model\n",
    "\n",
    "4. What is Beta-VAE?\n",
    "   - a) Basic VAE\n",
    "   - b) Disentanglement model\n",
    "   - c) Classification model\n",
    "   - d) Regression model\n",
    "\n",
    "5. What is cycle consistency?\n",
    "   - a) Data processing\n",
    "   - b) Translation constraint\n",
    "   - c) Model architecture\n",
    "   - d) Loss function\n",
    "\n",
    "6. What is disentanglement?\n",
    "   - a) Data processing\n",
    "   - b) Feature separation\n",
    "   - c) Model architecture\n",
    "   - d) Loss function\n",
    "\n",
    "7. What is style transfer?\n",
    "   - a) Data processing\n",
    "   - b) Style application\n",
    "   - c) Model architecture\n",
    "   - d) Loss function\n",
    "\n",
    "8. What is the purpose of skip connections?\n",
    "   - a) Model training\n",
    "   - b) Detail preservation\n",
    "   - c) Loss calculation\n",
    "   - d) Data processing\n",
    "\n",
    "9. What is perceptual loss?\n",
    "   - a) Basic loss\n",
    "   - b) Feature-based loss\n",
    "   - c) Classification loss\n",
    "   - d) Regression loss\n",
    "\n",
    "10. What is domain adaptation?\n",
    "    - a) Data processing\n",
    "    - b) Domain transfer\n",
    "    - c) Model architecture\n",
    "    - d) Loss function\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}