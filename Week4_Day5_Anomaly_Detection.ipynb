{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4, Day 5: Anomaly Detection\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand anomaly detection concepts\n",
    "- Learn different detection methods\n",
    "- Master outlier analysis techniques\n",
    "- Practice implementing anomaly detection\n",
    "\n",
    "## Topics Covered\n",
    "1. Statistical Methods\n",
    "2. Isolation Forest\n",
    "3. One-Class SVM\n",
    "4. Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def statistical_methods():\n",
    "    # Generate data with outliers\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    # Normal data\n",
    "    X_normal = np.random.normal(0, 1, (n_samples, 2))\n",
    "    \n",
    "    # Add outliers\n",
    "    X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "    X = np.vstack([X_normal, X_outliers])\n",
    "    \n",
    "    # Z-score method\n",
    "    z_scores = np.abs(stats.zscore(X))\n",
    "    outliers_z = np.any(z_scores > 3, axis=1)\n",
    "    \n",
    "    # IQR method\n",
    "    Q1 = np.percentile(X, 25, axis=0)\n",
    "    Q3 = np.percentile(X, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_iqr = np.any((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR)), axis=1)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original data\n",
    "    plt.subplot(131)\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "    plt.title('Original Data')\n",
    "    \n",
    "    # Z-score method\n",
    "    plt.subplot(132)\n",
    "    plt.scatter(X[~outliers_z, 0], X[~outliers_z, 1], label='Normal')\n",
    "    plt.scatter(X[outliers_z, 0], X[outliers_z, 1], color='red', label='Outlier')\n",
    "    plt.title('Z-score Method')\n",
    "    plt.legend()\n",
    "    \n",
    "    # IQR method\n",
    "    plt.subplot(133)\n",
    "    plt.scatter(X[~outliers_iqr, 0], X[~outliers_iqr, 1], label='Normal')\n",
    "    plt.scatter(X[outliers_iqr, 0], X[outliers_iqr, 1], color='red', label='Outlier')\n",
    "    plt.title('IQR Method')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "statistical_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def isolation_forest_example():\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    # Create normal samples\n",
    "    X_normal = np.random.normal(0, 1, (n_samples, 2))\n",
    "    \n",
    "    # Add outliers\n",
    "    X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "    X = np.vstack([X_normal, X_outliers])\n",
    "    \n",
    "    # Apply Isolation Forest\n",
    "    clf = IsolationForest(random_state=42, contamination=0.1)\n",
    "    y_pred = clf.fit_predict(X)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label='Normal')\n",
    "    plt.scatter(X[y_pred == -1, 0], X[y_pred == -1, 1], color='red', label='Outlier')\n",
    "    plt.title('Isolation Forest Results')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Number of outliers:\", (y_pred == -1).sum())\n",
    "    print(\"Number of normal points:\", (y_pred == 1).sum())\n",
    "\n",
    "isolation_forest_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def oneclass_svm_example():\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    # Create normal samples\n",
    "    X_normal = np.random.normal(0, 1, (n_samples, 2))\n",
    "    \n",
    "    # Add outliers\n",
    "    X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "    X = np.vstack([X_normal, X_outliers])\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply One-Class SVM\n",
    "    clf = OneClassSVM(kernel='rbf', nu=0.1)\n",
    "    y_pred = clf.fit_predict(X_scaled)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label='Normal')\n",
    "    plt.scatter(X[y_pred == -1, 0], X[y_pred == -1, 1], color='red', label='Outlier')\n",
    "    plt.title('One-Class SVM Results')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Number of outliers:\", (y_pred == -1).sum())\n",
    "    print(\"Number of normal points:\", (y_pred == 1).sum())\n",
    "\n",
    "oneclass_svm_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def lof_example():\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    # Create normal samples\n",
    "    X_normal = np.random.normal(0, 1, (n_samples, 2))\n",
    "    \n",
    "    # Add outliers\n",
    "    X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "    X = np.vstack([X_normal, X_outliers])\n",
    "    \n",
    "    # Apply LOF\n",
    "    clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "    y_pred = clf.fit_predict(X)\n",
    "    \n",
    "    # Get LOF scores\n",
    "    lof_scores = -clf.negative_outlier_factor_\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Points colored by prediction\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X[y_pred == 1, 0], X[y_pred == 1, 1], label='Normal')\n",
    "    plt.scatter(X[y_pred == -1, 0], X[y_pred == -1, 1], color='red', label='Outlier')\n",
    "    plt.title('LOF Classifications')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Points colored by LOF score\n",
    "    plt.subplot(122)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=lof_scores, cmap='viridis')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.title('LOF Scores')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Number of outliers:\", (y_pred == -1).sum())\n",
    "    print(\"Number of normal points:\", (y_pred == 1).sum())\n",
    "\n",
    "lof_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Credit Card Fraud Detection\n",
    "\n",
    "def credit_card_fraud():\n",
    "    # Generate synthetic credit card data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create normal transactions\n",
    "    amount_normal = np.random.lognormal(4, 0.5, n_samples)\n",
    "    time_normal = np.random.uniform(0, 24, n_samples)\n",
    "    \n",
    "    # Create fraudulent transactions\n",
    "    amount_fraud = np.random.lognormal(6, 1, 50)\n",
    "    time_fraud = np.random.uniform(0, 24, 50)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.column_stack([\n",
    "        np.concatenate([amount_normal, amount_fraud]),\n",
    "        np.concatenate([time_normal, time_fraud])\n",
    "    ])\n",
    "    \n",
    "    print(\"Dataset shape:\", X.shape)\n",
    "    \n",
    "    # Task: Detect fraudulent transactions\n",
    "    # 1. Scale the features\n",
    "    # 2. Apply multiple detection methods\n",
    "    # 3. Compare results\n",
    "    # 4. Analyze detection performance\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "credit_card_fraud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Network Intrusion Detection\n",
    "\n",
    "def network_intrusion():\n",
    "    # Generate synthetic network data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Normal network traffic\n",
    "    traffic_normal = np.random.normal(100, 20, n_samples)  # bytes/s\n",
    "    latency_normal = np.random.normal(50, 10, n_samples)   # ms\n",
    "    packets_normal = np.random.poisson(100, n_samples)     # packets/s\n",
    "    \n",
    "    # Anomalous traffic (DDoS attack)\n",
    "    traffic_anomaly = np.random.normal(500, 50, 50)\n",
    "    latency_anomaly = np.random.normal(200, 30, 50)\n",
    "    packets_anomaly = np.random.poisson(1000, 50)\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.column_stack([\n",
    "        np.concatenate([traffic_normal, traffic_anomaly]),\n",
    "        np.concatenate([latency_normal, latency_anomaly]),\n",
    "        np.concatenate([packets_normal, packets_anomaly])\n",
    "    ])\n",
    "    \n",
    "    print(\"Dataset shape:\", X.shape)\n",
    "    \n",
    "    # Task: Detect network anomalies\n",
    "    # 1. Preprocess the data\n",
    "    # 2. Implement detection methods\n",
    "    # 3. Visualize results\n",
    "    # 4. Compare detection methods\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "network_intrusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. Which method is best for high-dimensional data?\n",
    "   - a) Z-score\n",
    "   - b) IQR\n",
    "   - c) Isolation Forest\n",
    "   - d) LOF\n",
    "\n",
    "2. What is the main advantage of Isolation Forest?\n",
    "   - a) Linear complexity\n",
    "   - b) No parameters\n",
    "   - c) Perfect accuracy\n",
    "   - d) Works with categorical data\n",
    "\n",
    "3. What does the contamination parameter control?\n",
    "   - a) Training speed\n",
    "   - b) Expected proportion of outliers\n",
    "   - c) Number of features\n",
    "   - d) Model complexity\n",
    "\n",
    "4. Which method is density-based?\n",
    "   - a) Isolation Forest\n",
    "   - b) One-Class SVM\n",
    "   - c) LOF\n",
    "   - d) Z-score\n",
    "\n",
    "5. What is the time complexity of LOF?\n",
    "   - a) O(n)\n",
    "   - b) O(n log n)\n",
    "   - c) O(n²)\n",
    "   - d) O(n³)\n",
    "\n",
    "6. Which method requires feature scaling?\n",
    "   - a) Isolation Forest\n",
    "   - b) One-Class SVM\n",
    "   - c) IQR\n",
    "   - d) Z-score\n",
    "\n",
    "7. What is the main limitation of statistical methods?\n",
    "   - a) Slow computation\n",
    "   - b) High memory usage\n",
    "   - c) Assumes normal distribution\n",
    "   - d) Complex implementation\n",
    "\n",
    "8. Which method is most suitable for streaming data?\n",
    "   - a) LOF\n",
    "   - b) One-Class SVM\n",
    "   - c) Statistical methods\n",
    "   - d) Isolation Forest\n",
    "\n",
    "9. What does One-Class SVM learn?\n",
    "   - a) Cluster centers\n",
    "   - b) Decision boundary\n",
    "   - c) Feature importance\n",
    "   - d) Distance metrics\n",
    "\n",
    "10. Which is NOT an application of anomaly detection?\n",
    "    - a) Fraud detection\n",
    "    - b) Network security\n",
    "    - c) Image classification\n",
    "    - d) System monitoring\n",
    "\n",
    "Answers: 1-c, 2-a, 3-b, 4-c, 5-c, 6-b, 7-c, 8-c, 9-b, 10-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}