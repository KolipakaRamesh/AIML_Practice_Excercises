{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7, Day 5: Image Generation and Style Transfer\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand generative models\n",
    "- Learn style transfer techniques\n",
    "- Master GAN architectures\n",
    "- Practice implementing generators\n",
    "\n",
    "## Topics Covered\n",
    "1. Generative Models\n",
    "2. Style Transfer\n",
    "3. GANs\n",
    "4. Image Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def style_transfer_example():\n",
    "    # Load VGG19 model\n",
    "    vgg = VGG19(include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Content and style layers\n",
    "    content_layers = ['block5_conv2']\n",
    "    style_layers = ['block1_conv1',\n",
    "                    'block2_conv1',\n",
    "                    'block3_conv1',\n",
    "                    'block4_conv1',\n",
    "                    'block5_conv1']\n",
    "    \n",
    "    # Create model\n",
    "    def get_model():\n",
    "        vgg.trainable = False\n",
    "        \n",
    "        # Get outputs\n",
    "        style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "        content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "        model_outputs = style_outputs + content_outputs\n",
    "        \n",
    "        return Model(vgg.input, model_outputs)\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    def load_img(path_to_img):\n",
    "        img = tf.io.read_file(path_to_img)\n",
    "        img = tf.image.decode_image(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = tf.image.resize(img, [224, 224])\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    # Example usage\n",
    "    content_path = 'content.jpg'\n",
    "    style_path = 'style.jpg'\n",
    "    \n",
    "    content_image = load_img(content_path)\n",
    "    style_image = load_img(style_path)\n",
    "    \n",
    "    # Display images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(content_image[0])\n",
    "    plt.title('Content Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(style_image[0])\n",
    "    plt.title('Style Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "style_transfer_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gan_example():\n",
    "    # Generator model\n",
    "    def make_generator():\n",
    "        model = tf.keras.Sequential([\n",
    "            Dense(7*7*256, use_bias=False, input_shape=(100,)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            \n",
    "            tf.keras.layers.Reshape((7, 7, 256)),\n",
    "            \n",
    "            Conv2DTranspose(128, 5, strides=1, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            \n",
    "            Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            \n",
    "            Conv2DTranspose(1, 5, strides=2, padding='same', use_bias=False,\n",
    "                           activation='tanh')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Discriminator model\n",
    "    def make_discriminator():\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv2D(64, 5, strides=2, padding='same',\n",
    "                   input_shape=[28, 28, 1]),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            \n",
    "            Conv2D(128, 5, strides=2, padding='same'),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            \n",
    "            tf.keras.layers.Flatten(),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Create models\n",
    "    generator = make_generator()\n",
    "    discriminator = make_discriminator()\n",
    "    \n",
    "    # Generate sample images\n",
    "    noise = tf.random.normal([16, 100])\n",
    "    generated_images = generator(noise, training=False)\n",
    "    \n",
    "    # Display generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "gan_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image-to-Image Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def pix2pix_example():\n",
    "    # Generator\n",
    "    def make_generator():\n",
    "        inputs = Input(shape=[256, 256, 3])\n",
    "        \n",
    "        # Encoder\n",
    "        down_stack = [\n",
    "            Conv2D(64, 4, strides=2, padding='same'),\n",
    "            Conv2D(128, 4, strides=2, padding='same'),\n",
    "            Conv2D(256, 4, strides=2, padding='same'),\n",
    "            Conv2D(512, 4, strides=2, padding='same'),\n",
    "        ]\n",
    "        \n",
    "        # Decoder\n",
    "        up_stack = [\n",
    "            Conv2DTranspose(256, 4, strides=2, padding='same'),\n",
    "            Conv2DTranspose(128, 4, strides=2, padding='same'),\n",
    "            Conv2DTranspose(64, 4, strides=2, padding='same'),\n",
    "        ]\n",
    "        \n",
    "        # Final layer\n",
    "        last = Conv2DTranspose(3, 4, strides=2, padding='same')\n",
    "        \n",
    "        x = inputs\n",
    "        \n",
    "        # Downsampling\n",
    "        skips = []\n",
    "        for down in down_stack:\n",
    "            x = down(x)\n",
    "            skips.append(x)\n",
    "        \n",
    "        skips = reversed(skips[:-1])\n",
    "        \n",
    "        # Upsampling and establishing skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip])\n",
    "        \n",
    "        x = last(x)\n",
    "        \n",
    "        return Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    # Create model\n",
    "    generator = make_generator()\n",
    "    \n",
    "    # Example usage\n",
    "    sample_input = tf.random.normal([1, 256, 256, 3])\n",
    "    generated_image = generator(sample_input, training=False)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(sample_input[0] * 0.5 + 0.5)\n",
    "    plt.title('Input')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(generated_image[0] * 0.5 + 0.5)\n",
    "    plt.title('Generated')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "pix2pix_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Style Transfer Implementation\n",
    "\n",
    "def style_transfer_exercise():\n",
    "    print(\"Task: Implement neural style transfer\")\n",
    "    print(\"1. Load content and style images\")\n",
    "    print(\"2. Extract features\")\n",
    "    print(\"3. Compute losses\")\n",
    "    print(\"4. Generate stylized image\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "style_transfer_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Simple GAN\n",
    "\n",
    "def gan_exercise():\n",
    "    print(\"Task: Implement a basic GAN\")\n",
    "    print(\"1. Create generator\")\n",
    "    print(\"2. Create discriminator\")\n",
    "    print(\"3. Train models\")\n",
    "    print(\"4. Generate images\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "gan_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is style transfer?\n",
    "   - a) Image classification\n",
    "   - b) Style application\n",
    "   - c) Object detection\n",
    "   - d) Image segmentation\n",
    "\n",
    "2. What is a GAN?\n",
    "   - a) Classification model\n",
    "   - b) Generative model\n",
    "   - c) Segmentation model\n",
    "   - d) Detection model\n",
    "\n",
    "3. What is the generator in GAN?\n",
    "   - a) Classifier\n",
    "   - b) Image creator\n",
    "   - c) Feature extractor\n",
    "   - d) Data loader\n",
    "\n",
    "4. What is the discriminator in GAN?\n",
    "   - a) Generator\n",
    "   - b) Fake detector\n",
    "   - c) Data loader\n",
    "   - d) Image processor\n",
    "\n",
    "5. What is content loss?\n",
    "   - a) Training error\n",
    "   - b) Feature difference\n",
    "   - c) Model size\n",
    "   - d) Image size\n",
    "\n",
    "6. What is style loss?\n",
    "   - a) Training error\n",
    "   - b) Texture difference\n",
    "   - c) Model size\n",
    "   - d) Image size\n",
    "\n",
    "7. What is mode collapse?\n",
    "   - a) Model error\n",
    "   - b) GAN problem\n",
    "   - c) Training method\n",
    "   - d) Loss function\n",
    "\n",
    "8. What is cycle consistency?\n",
    "   - a) Training method\n",
    "   - b) Translation constraint\n",
    "   - c) Loss function\n",
    "   - d) Model architecture\n",
    "\n",
    "9. What is perceptual loss?\n",
    "   - a) Training error\n",
    "   - b) Feature-based loss\n",
    "   - c) Model size\n",
    "   - d) Image size\n",
    "\n",
    "10. What is progressive growing?\n",
    "    - a) Training method\n",
    "    - b) Resolution increase\n",
    "    - c) Loss function\n",
    "    - d) Model architecture\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}