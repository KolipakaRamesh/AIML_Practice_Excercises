{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7, Day 6: Computer Vision Hackathon Challenge\n",
    "\n",
    "## Challenge Overview\n",
    "Build an end-to-end computer vision solution using concepts learned throughout Week 7:\n",
    "- Image Processing\n",
    "- Object Detection\n",
    "- Image Segmentation\n",
    "- Face Analysis\n",
    "- Image Generation\n",
    "\n",
    "## Problem: Multi-Task Vision System\n",
    "Create a system that can perform multiple computer vision tasks:\n",
    "1. Object Detection and Recognition\n",
    "2. Scene Segmentation\n",
    "3. Face Analysis\n",
    "4. Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess dataset\"\"\"\n",
    "    # Load images\n",
    "    image_paths = [\n",
    "        'street_scene1.jpg',\n",
    "        'street_scene2.jpg',\n",
    "        'indoor_scene1.jpg',\n",
    "        'indoor_scene2.jpg'\n",
    "    ]\n",
    "    \n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        # Read image\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        \n",
    "        # Normalize\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        images.append(img)\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "# Load data\n",
    "images = load_and_preprocess_data()\n",
    "\n",
    "# Display sample images\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(min(4, len(images))):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f'Image {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_object_detection():\n",
    "    \"\"\"Implement object detection system\"\"\"\n",
    "    # Load pre-trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    def detect_objects(image):\n",
    "        # Convert to torch tensor\n",
    "        img_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model(img_tensor)\n",
    "        \n",
    "        return predictions[0]\n",
    "    \n",
    "    # Process sample image\n",
    "    results = detect_objects(images[0])\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(images[0])\n",
    "    \n",
    "    # Draw boxes\n",
    "    for box, score, label in zip(results['boxes'], results['scores'], results['labels']):\n",
    "        if score > 0.5:  # Confidence threshold\n",
    "            x1, y1, x2, y2 = box.numpy()\n",
    "            plt.gca().add_patch(plt.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                fill=False, color='red', linewidth=2\n",
    "            ))\n",
    "    \n",
    "    plt.title('Object Detection Results')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "implement_object_detection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Scene Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_segmentation():\n",
    "    \"\"\"Implement semantic segmentation\"\"\"\n",
    "    # Load pre-trained model\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    def segment_image(image):\n",
    "        # Preprocess\n",
    "        img_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)['out'][0]\n",
    "        \n",
    "        return output.argmax(0).numpy()\n",
    "    \n",
    "    # Process sample image\n",
    "    segmentation_map = segment_image(images[0])\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[0])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(segmentation_map)\n",
    "    plt.title('Segmentation Map')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "implement_segmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Face Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_face_analysis():\n",
    "    \"\"\"Implement face analysis system\"\"\"\n",
    "    # Load face cascade\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    )\n",
    "    \n",
    "    def analyze_face(image):\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        return faces\n",
    "    \n",
    "    # Process sample image\n",
    "    faces = analyze_face(images[0])\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(images[0])\n",
    "    \n",
    "    # Draw rectangles around faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        plt.gca().add_patch(plt.Rectangle(\n",
    "            (x, y), w, h,\n",
    "            fill=False, color='green', linewidth=2\n",
    "        ))\n",
    "    \n",
    "    plt.title('Face Detection Results')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "implement_face_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def implement_enhancement():\n",
    "    \"\"\"Implement image enhancement\"\"\"\n",
    "    def enhance_image(image):\n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Split channels\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Apply CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl = clahe.apply(l)\n",
    "        \n",
    "        # Merge channels\n",
    "        limg = cv2.merge((cl,a,b))\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        enhanced = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        return enhanced / 255.0\n",
    "    \n",
    "    # Process sample image\n",
    "    enhanced_image = enhance_image(images[0])\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[0])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(enhanced_image)\n",
    "    plt.title('Enhanced Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "implement_enhancement()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "Your solution will be evaluated based on:\n",
    "\n",
    "1. Object Detection (25%)\n",
    "   - Detection accuracy\n",
    "   - Processing speed\n",
    "   - Implementation quality\n",
    "\n",
    "2. Segmentation (25%)\n",
    "   - Segmentation accuracy\n",
    "   - Edge precision\n",
    "   - Class coverage\n",
    "\n",
    "3. Face Analysis (25%)\n",
    "   - Detection accuracy\n",
    "   - Feature extraction\n",
    "   - Analysis depth\n",
    "\n",
    "4. Enhancement (25%)\n",
    "   - Image quality\n",
    "   - Processing efficiency\n",
    "   - Result naturalness\n",
    "\n",
    "## Submission Guidelines\n",
    "1. Complete all tasks in this notebook\n",
    "2. Document your approach and decisions\n",
    "3. Include visualizations and analysis\n",
    "4. Provide suggestions for improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}