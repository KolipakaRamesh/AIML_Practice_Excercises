{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6, Day 5: Language Models and Text Generation\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand language model concepts\n",
    "- Learn text generation techniques\n",
    "- Master sequence modeling\n",
    "- Practice implementing language models\n",
    "\n",
    "## Topics Covered\n",
    "1. Language Model Fundamentals\n",
    "2. Text Generation Methods\n",
    "3. Sequence-to-Sequence Models\n",
    "4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def ngram_model_example():\n",
    "    # Sample text\n",
    "    text = \"\"\"\n",
    "    Natural language processing is a field of artificial intelligence.\n",
    "    Language models help computers understand and generate human text.\n",
    "    Modern language models use deep learning techniques.\n",
    "    These models can generate coherent and contextually relevant text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Create n-grams\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    def calculate_ngram_probs(ngrams):\n",
    "        ngram_freq = {}\n",
    "        for ngram in ngrams:\n",
    "            if ngram in ngram_freq:\n",
    "                ngram_freq[ngram] += 1\n",
    "            else:\n",
    "                ngram_freq[ngram] = 1\n",
    "        return ngram_freq\n",
    "    \n",
    "    bigram_probs = calculate_ngram_probs(bigrams)\n",
    "    trigram_probs = calculate_ngram_probs(trigrams)\n",
    "    \n",
    "    # Print most common n-grams\n",
    "    print(\"Most Common Bigrams:\")\n",
    "    for bigram, freq in sorted(bigram_probs.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{bigram}: {freq}\")\n",
    "    \n",
    "    print(\"\\nMost Common Trigrams:\")\n",
    "    for trigram, freq in sorted(trigram_probs.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"{trigram}: {freq}\")\n",
    "    \n",
    "    # Visualize n-gram distributions\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.bar(range(len(bigram_probs)), sorted(bigram_probs.values(), reverse=True))\n",
    "    plt.title('Bigram Distribution')\n",
    "    plt.xlabel('Bigram Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.bar(range(len(trigram_probs)), sorted(trigram_probs.values(), reverse=True))\n",
    "    plt.title('Trigram Distribution')\n",
    "    plt.xlabel('Trigram Rank')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "ngram_model_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def neural_lm_example():\n",
    "    # Sample sentences\n",
    "    sentences = [\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The dog ran in the park\",\n",
    "        \"A bird flew over the tree\",\n",
    "        \"The sun shines in the sky\",\n",
    "        \"A fish swims in the sea\"\n",
    "    ]\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Create input sequences\n",
    "    input_sequences = []\n",
    "    for sentence in sentences:\n",
    "        token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "    \n",
    "    # Create predictors and target\n",
    "    X = input_sequences[:, :-1]\n",
    "    y = input_sequences[:, -1]\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential([\n",
    "        Embedding(total_words, 16, input_length=max_sequence_len-1),\n",
    "        LSTM(32),\n",
    "        Dense(total_words, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X, y, epochs=100, verbose=0)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate text\n",
    "    def generate_text(seed_text, next_words=5):\n",
    "        for _ in range(next_words):\n",
    "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "            predicted = model.predict(token_list, verbose=0)\n",
    "            predicted = np.argmax(predicted, axis=-1)\n",
    "            output_word = \"\"\n",
    "            for word, index in tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \" + output_word\n",
    "        return seed_text\n",
    "    \n",
    "    # Test text generation\n",
    "    print(\"\\nGenerated Text Examples:\")\n",
    "    print(generate_text(\"The cat\"))\n",
    "    print(generate_text(\"A bird\"))\n",
    "\n",
    "neural_lm_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def text_generation_example():\n",
    "    # Sample text corpus\n",
    "    text = \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "    A quick brown dog jumps over the lazy fox.\n",
    "    The lazy brown fox sleeps under the quick dog.\n",
    "    A lazy dog sleeps under the quick brown fox.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokenizer = Tokenizer(char_level=True)\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    total_chars = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    # Create character sequences\n",
    "    input_sequences = []\n",
    "    for line in text.split('\\n'):\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "    \n",
    "    # Create predictors and target\n",
    "    X = input_sequences[:, :-1]\n",
    "    y = input_sequences[:, -1]\n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=total_chars)\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential([\n",
    "        Embedding(total_chars, 16, input_length=max_sequence_len-1),\n",
    "        LSTM(32, return_sequences=True),\n",
    "        LSTM(32),\n",
    "        Dense(total_chars, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X, y, epochs=100, verbose=0)\n",
    "    \n",
    "    # Generate text\n",
    "    def generate_text(seed_text, next_chars=50):\n",
    "        for _ in range(next_chars):\n",
    "            token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "            predicted = model.predict(token_list, verbose=0)\n",
    "            predicted = np.argmax(predicted, axis=-1)\n",
    "            output_char = \"\"\n",
    "            for char, index in tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_char = char\n",
    "                    break\n",
    "            seed_text += output_char\n",
    "        return seed_text\n",
    "    \n",
    "    # Test text generation\n",
    "    print(\"Generated Text:\")\n",
    "    print(generate_text(\"The quick\"))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "text_generation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Custom Language Model\n",
    "\n",
    "def language_model_exercise():\n",
    "    # Sample text corpus\n",
    "    corpus = [\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"Deep learning models can process complex data.\",\n",
    "        \"Neural networks are inspired by biological systems.\",\n",
    "        \"Data science combines statistics and programming.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Task: Build a custom language model\")\n",
    "    print(\"1. Implement text preprocessing\")\n",
    "    print(\"2. Create model architecture\")\n",
    "    print(\"3. Train the model\")\n",
    "    print(\"4. Generate text\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "language_model_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Text Style Transfer\n",
    "\n",
    "def style_transfer_exercise():\n",
    "    # Sample texts in different styles\n",
    "    formal_texts = [\n",
    "        \"The meeting is scheduled for tomorrow afternoon.\",\n",
    "        \"Please find attached the requested documents.\",\n",
    "        \"We look forward to your response.\"\n",
    "    ]\n",
    "    \n",
    "    informal_texts = [\n",
    "        \"Hey, let's meet up tomorrow!\",\n",
    "        \"Here are the docs you wanted.\",\n",
    "        \"Can't wait to hear back from you!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Task: Implement text style transfer\")\n",
    "    print(\"1. Create style embeddings\")\n",
    "    print(\"2. Build transfer model\")\n",
    "    print(\"3. Transform text\")\n",
    "    print(\"4. Evaluate results\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "style_transfer_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is a language model?\n",
    "   - a) Translation system\n",
    "   - b) Probability distribution over text\n",
    "   - c) Text editor\n",
    "   - d) Speech recognition\n",
    "\n",
    "2. What is perplexity?\n",
    "   - a) Model size\n",
    "   - b) Model performance measure\n",
    "   - c) Text length\n",
    "   - d) Training time\n",
    "\n",
    "3. What are n-grams?\n",
    "   - a) Neural networks\n",
    "   - b) Sequence of n items\n",
    "   - c) Text classification\n",
    "   - d) Model parameters\n",
    "\n",
    "4. What is beam search?\n",
    "   - a) Data preprocessing\n",
    "   - b) Text generation method\n",
    "   - c) Model architecture\n",
    "   - d) Training algorithm\n",
    "\n",
    "5. What is temperature in text generation?\n",
    "   - a) Hardware metric\n",
    "   - b) Randomness parameter\n",
    "   - c) Model size\n",
    "   - d) Training time\n",
    "\n",
    "6. What is the purpose of tokenization?\n",
    "   - a) Text generation\n",
    "   - b) Text to discrete units\n",
    "   - c) Model training\n",
    "   - d) Evaluation\n",
    "\n",
    "7. What is sequence-to-sequence model?\n",
    "   - a) Classification model\n",
    "   - b) Sequence transformation\n",
    "   - c) Clustering algorithm\n",
    "   - d) Visualization tool\n",
    "\n",
    "8. What is attention mechanism?\n",
    "   - a) Training method\n",
    "   - b) Focus on relevant parts\n",
    "   - c) Data preprocessing\n",
    "   - d) Model evaluation\n",
    "\n",
    "9. What is teacher forcing?\n",
    "   - a) Model architecture\n",
    "   - b) Training technique\n",
    "   - c) Evaluation metric\n",
    "   - d) Data augmentation\n",
    "\n",
    "10. What is the purpose of sampling strategies?\n",
    "    - a) Data collection\n",
    "    - b) Text generation control\n",
    "    - c) Model training\n",
    "    - d) Error analysis\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}