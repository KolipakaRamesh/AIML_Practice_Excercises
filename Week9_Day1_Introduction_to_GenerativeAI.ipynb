{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9, Day 1: Introduction to Generative AI\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand generative AI concepts\n",
    "- Learn different types of generative models\n",
    "- Master basic generation techniques\n",
    "- Practice implementing generators\n",
    "\n",
    "## Topics Covered\n",
    "1. Generative Models Overview\n",
    "2. Variational Autoencoders (VAEs)\n",
    "3. Generative Adversarial Networks (GANs)\n",
    "4. Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2D(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim + latent_dim)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(latent_dim,)),\n",
    "            layers.Dense(7*7*32, activation='relu'),\n",
    "            layers.Reshape((7, 7, 32)),\n",
    "            layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same'),\n",
    "            layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "        return x_logit, mean, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generative Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GAN:\n",
    "    def __init__(self, latent_dim):\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Generator\n",
    "        self.generator = tf.keras.Sequential([\n",
    "            layers.Input(shape=(latent_dim,)),\n",
    "            layers.Dense(7*7*256, use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Reshape((7, 7, 256)),\n",
    "            layers.Conv2DTranspose(128, 5, strides=1, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Conv2DTranspose(1, 5, strides=2, padding='same', use_bias=False,\n",
    "                                  activation='tanh')\n",
    "        ])\n",
    "        \n",
    "        # Discriminator\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(64, 5, strides=2, padding='same'),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Conv2D(128, 5, strides=2, padding='same'),\n",
    "            layers.LeakyReLU(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Optimizers\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "    def generator_loss(self, fake_output):\n",
    "        return tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True\n",
    "        )(tf.ones_like(fake_output), fake_output)\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True\n",
    "        )(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True\n",
    "        )(tf.zeros_like(fake_output), fake_output)\n",
    "        return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self, timesteps=1000):\n",
    "        self.timesteps = timesteps\n",
    "        self.beta = np.linspace(0.0001, 0.02, timesteps)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_bar = np.cumprod(self.alpha)\n",
    "        \n",
    "        # Noise predictor network\n",
    "        self.model = tf.keras.Sequential([\n",
    "            layers.Input(shape=(28, 28, 1)),\n",
    "            layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            layers.Conv2D(256, 3, activation='relu', padding='same'),\n",
    "            layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "            layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "            layers.Conv2D(1, 3, padding='same')\n",
    "        ])\n",
    "    \n",
    "    def forward_diffusion(self, x0, t):\n",
    "        noise = tf.random.normal(shape=x0.shape)\n",
    "        alpha_t = tf.gather(self.alpha_bar, t)\n",
    "        alpha_t = tf.reshape(alpha_t, (-1, 1, 1, 1))\n",
    "        return tf.sqrt(alpha_t) * x0 + tf.sqrt(1. - alpha_t) * noise, noise\n",
    "    \n",
    "    def reverse_diffusion(self, x, t):\n",
    "        predicted_noise = self.model(x, training=False)\n",
    "        alpha_t = tf.gather(self.alpha_bar, t)\n",
    "        alpha_t = tf.reshape(alpha_t, (-1, 1, 1, 1))\n",
    "        beta_t = tf.gather(self.beta, t)\n",
    "        beta_t = tf.reshape(beta_t, (-1, 1, 1, 1))\n",
    "        \n",
    "        mean = (1. / tf.sqrt(alpha_t)) * (x - ((1. - alpha_t) / tf.sqrt(1. - alpha_t)) * predicted_noise)\n",
    "        variance = beta_t\n",
    "        \n",
    "        return mean + tf.sqrt(variance) * tf.random.normal(shape=x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Simple VAE\n",
    "\n",
    "def vae_exercise():\n",
    "    print(\"Task: Implement a basic VAE\")\n",
    "    print(\"1. Create encoder\")\n",
    "    print(\"2. Create decoder\")\n",
    "    print(\"3. Train model\")\n",
    "    print(\"4. Generate samples\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "vae_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Simple GAN\n",
    "\n",
    "def gan_exercise():\n",
    "    print(\"Task: Implement a basic GAN\")\n",
    "    print(\"1. Create generator\")\n",
    "    print(\"2. Create discriminator\")\n",
    "    print(\"3. Train adversarial model\")\n",
    "    print(\"4. Generate samples\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "gan_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is generative AI?\n",
    "   - a) Classification model\n",
    "   - b) Content creation model\n",
    "   - c) Regression model\n",
    "   - d) Clustering model\n",
    "\n",
    "2. What is a VAE?\n",
    "   - a) Classification model\n",
    "   - b) Generative model\n",
    "   - c) Regression model\n",
    "   - d) Clustering model\n",
    "\n",
    "3. What is a GAN?\n",
    "   - a) Single network\n",
    "   - b) Adversarial networks\n",
    "   - c) Classification model\n",
    "   - d) Regression model\n",
    "\n",
    "4. What is a diffusion model?\n",
    "   - a) Classification model\n",
    "   - b) Noise-based generation\n",
    "   - c) Regression model\n",
    "   - d) Clustering model\n",
    "\n",
    "5. What is the latent space?\n",
    "   - a) Input space\n",
    "   - b) Compressed representation\n",
    "   - c) Output space\n",
    "   - d) Feature space\n",
    "\n",
    "6. What is the role of the discriminator?\n",
    "   - a) Generate samples\n",
    "   - b) Detect fake samples\n",
    "   - c) Compress data\n",
    "   - d) Process input\n",
    "\n",
    "7. What is the purpose of reparameterization?\n",
    "   - a) Data processing\n",
    "   - b) Backpropagation trick\n",
    "   - c) Model evaluation\n",
    "   - d) Data augmentation\n",
    "\n",
    "8. What is mode collapse?\n",
    "   - a) Model success\n",
    "   - b) GAN problem\n",
    "   - c) Training method\n",
    "   - d) Evaluation metric\n",
    "\n",
    "9. What is the ELBO loss?\n",
    "   - a) Classification loss\n",
    "   - b) VAE objective\n",
    "   - c) GAN loss\n",
    "   - d) Regression loss\n",
    "\n",
    "10. What is denoising diffusion?\n",
    "    - a) Data cleaning\n",
    "    - b) Generation process\n",
    "    - c) Model architecture\n",
    "    - d) Loss function\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}