{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8, Day 5: Advanced Reinforcement Learning Topics\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand advanced RL concepts\n",
    "- Learn multi-agent RL\n",
    "- Master hierarchical RL\n",
    "- Practice implementing advanced techniques\n",
    "\n",
    "## Topics Covered\n",
    "1. Multi-Agent RL\n",
    "2. Hierarchical RL\n",
    "3. Meta-Learning\n",
    "4. Exploration Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Agent RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiAgentDQN:\n",
    "    def __init__(self, state_size, action_size, n_agents):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_agents = n_agents\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.agents = [self._build_model() for _ in range(n_agents)]\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, actions, rewards, next_state, done):\n",
    "        self.memory.append((state, actions, rewards, next_state, done))\n",
    "    \n",
    "    def act(self, states):\n",
    "        actions = []\n",
    "        for i, state in enumerate(states):\n",
    "            if random.random() <= self.epsilon:\n",
    "                actions.append(random.randrange(self.action_size))\n",
    "            else:\n",
    "                act_values = self.agents[i].predict(state)\n",
    "                actions.append(np.argmax(act_values[0]))\n",
    "        return actions\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for agent_id in range(self.n_agents):\n",
    "            states = np.zeros((batch_size, self.state_size))\n",
    "            targets = np.zeros((batch_size, self.action_size))\n",
    "            \n",
    "            for i, (state, actions, rewards, next_state, done) in enumerate(minibatch):\n",
    "                target = rewards[agent_id]\n",
    "                if not done:\n",
    "                    target += self.gamma * np.amax(\n",
    "                        self.agents[agent_id].predict(next_state[agent_id])[0]\n",
    "                    )\n",
    "                \n",
    "                target_f = self.agents[agent_id].predict(state[agent_id])\n",
    "                target_f[0][actions[agent_id]] = target\n",
    "                \n",
    "                states[i] = state[agent_id]\n",
    "                targets[i] = target_f[0]\n",
    "            \n",
    "            self.agents[agent_id].fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HierarchicalRL:\n",
    "    def __init__(self, state_size, action_size, n_options=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.n_options = n_options\n",
    "        \n",
    "        # Meta-controller (selects options)\n",
    "        self.meta_controller = self._build_meta_controller()\n",
    "        \n",
    "        # Option policies\n",
    "        self.option_policies = [self._build_option_policy() for _ in range(n_options)]\n",
    "        \n",
    "        # Option termination conditions\n",
    "        self.termination_nets = [self._build_termination_net() for _ in range(n_options)]\n",
    "    \n",
    "    def _build_meta_controller(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.n_options, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    def _build_option_policy(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    def _build_termination_net(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    def select_option(self, state):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        option_probs = self.meta_controller.predict(state)[0]\n",
    "        return np.random.choice(self.n_options, p=option_probs)\n",
    "    \n",
    "    def select_action(self, state, option):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        action_probs = self.option_policies[option].predict(state)[0]\n",
    "        return np.random.choice(self.action_size, p=action_probs)\n",
    "    \n",
    "    def should_terminate(self, state, option):\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        return self.termination_nets[option].predict(state)[0, 0] > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MAMLAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.meta_lr = 0.01\n",
    "        self.task_lr = 0.1\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.meta_lr),\n",
    "                     loss='categorical_crossentropy')\n",
    "        return model\n",
    "    \n",
    "    def adapt_to_task(self, task_data):\n",
    "        # Create task-specific model\n",
    "        task_model = tf.keras.models.clone_model(self.model)\n",
    "        task_model.set_weights(self.model.get_weights())\n",
    "        task_model.compile(optimizer=tf.keras.optimizers.SGD(lr=self.task_lr),\n",
    "                          loss='categorical_crossentropy')\n",
    "        \n",
    "        # Adapt to task\n",
    "        states, actions = task_data\n",
    "        task_model.fit(states, actions, epochs=1, verbose=0)\n",
    "        \n",
    "        return task_model\n",
    "    \n",
    "    def meta_update(self, tasks_data):\n",
    "        meta_gradients = []\n",
    "        \n",
    "        for task_data in tasks_data:\n",
    "            # Compute task-specific gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                states, actions = task_data\n",
    "                predictions = self.model(states)\n",
    "                loss = tf.keras.losses.categorical_crossentropy(actions, predictions)\n",
    "            \n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            meta_gradients.append(gradients)\n",
    "        \n",
    "        # Average gradients across tasks\n",
    "        avg_gradients = [\n",
    "            tf.reduce_mean([g[i] for g in meta_gradients], axis=0)\n",
    "            for i in range(len(meta_gradients[0]))\n",
    "        ]\n",
    "        \n",
    "        # Apply meta-update\n",
    "        self.model.optimizer.apply_gradients(\n",
    "            zip(avg_gradients, self.model.trainable_variables)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NoveltySearch:\n",
    "    def __init__(self, state_size, k_neighbors=10):\n",
    "        self.state_size = state_size\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.archive = []\n",
    "    \n",
    "    def compute_novelty(self, state):\n",
    "        if len(self.archive) < self.k_neighbors:\n",
    "            return float('inf')\n",
    "        \n",
    "        distances = [np.linalg.norm(state - archived_state)\n",
    "                    for archived_state in self.archive]\n",
    "        distances.sort()\n",
    "        \n",
    "        return np.mean(distances[:self.k_neighbors])\n",
    "    \n",
    "    def update_archive(self, state, novelty_threshold=1.0):\n",
    "        novelty = self.compute_novelty(state)\n",
    "        if novelty > novelty_threshold:\n",
    "            self.archive.append(state)\n",
    "        \n",
    "        return novelty\n",
    "\n",
    "class IntrinsicMotivation:\n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "        self.predictor = self._build_predictor()\n",
    "        self.target = self._build_predictor()\n",
    "    \n",
    "    def _build_predictor(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_dim=self.state_size, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.state_size)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def compute_curiosity(self, state, next_state):\n",
    "        prediction = self.predictor.predict(state)\n",
    "        target_prediction = self.target.predict(state)\n",
    "        \n",
    "        prediction_error = np.mean(\n",
    "            (prediction - next_state) ** 2\n",
    "        )\n",
    "        disagreement = np.mean(\n",
    "            (prediction - target_prediction) ** 2\n",
    "        )\n",
    "        \n",
    "        return prediction_error + disagreement\n",
    "    \n",
    "    def update(self, state, next_state):\n",
    "        self.predictor.fit(state, next_state, verbose=0)\n",
    "        # Slow update of target network\n",
    "        target_weights = self.target.get_weights()\n",
    "        predictor_weights = self.predictor.get_weights()\n",
    "        new_weights = [0.95 * t + 0.05 * p\n",
    "                      for t, p in zip(target_weights, predictor_weights)]\n",
    "        self.target.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Multi-Agent System\n",
    "\n",
    "def multi_agent_exercise():\n",
    "    print(\"Task: Implement multi-agent coordination\")\n",
    "    print(\"1. Create agent network\")\n",
    "    print(\"2. Implement communication\")\n",
    "    print(\"3. Train agents\")\n",
    "    print(\"4. Evaluate performance\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "multi_agent_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Exploration Strategy\n",
    "\n",
    "def exploration_exercise():\n",
    "    print(\"Task: Implement advanced exploration\")\n",
    "    print(\"1. Design exploration metric\")\n",
    "    print(\"2. Implement search strategy\")\n",
    "    print(\"3. Test exploration\")\n",
    "    print(\"4. Analyze results\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "exploration_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is multi-agent RL?\n",
    "   - a) Single agent learning\n",
    "   - b) Multiple agent learning\n",
    "   - c) Model-based learning\n",
    "   - d) Supervised learning\n",
    "\n",
    "2. What is hierarchical RL?\n",
    "   - a) Flat policy\n",
    "   - b) Nested policies\n",
    "   - c) Single policy\n",
    "   - d) Random policy\n",
    "\n",
    "3. What is meta-learning?\n",
    "   - a) Single task learning\n",
    "   - b) Learning to learn\n",
    "   - c) Supervised learning\n",
    "   - d) Model-based learning\n",
    "\n",
    "4. What is novelty search?\n",
    "   - a) Random search\n",
    "   - b) Behavioral diversity\n",
    "   - c) Policy search\n",
    "   - d) Value search\n",
    "\n",
    "5. What is intrinsic motivation?\n",
    "   - a) External rewards\n",
    "   - b) Internal rewards\n",
    "   - c) Fixed rewards\n",
    "   - d) Random rewards\n",
    "\n",
    "6. What is option learning?\n",
    "   - a) Single action\n",
    "   - b) Temporal abstraction\n",
    "   - c) Random policy\n",
    "   - d) Value function\n",
    "\n",
    "7. What is curriculum learning?\n",
    "   - a) Random tasks\n",
    "   - b) Progressive tasks\n",
    "   - c) Single task\n",
    "   - d) Fixed task\n",
    "\n",
    "8. What is transfer learning in RL?\n",
    "   - a) Single task\n",
    "   - b) Knowledge reuse\n",
    "   - c) Random learning\n",
    "   - d) Fixed policy\n",
    "\n",
    "9. What is exploration vs exploitation?\n",
    "   - a) Random actions\n",
    "   - b) Learning tradeoff\n",
    "   - c) Fixed policy\n",
    "   - d) Value function\n",
    "\n",
    "10. What is model-based RL?\n",
    "    - a) Model-free learning\n",
    "    - b) Environment modeling\n",
    "    - c) Random policy\n",
    "    - d) Fixed policy\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}