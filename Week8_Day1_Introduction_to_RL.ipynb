{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8, Day 1: Introduction to Reinforcement Learning\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RL fundamentals\n",
    "- Learn key RL concepts\n",
    "- Master basic RL algorithms\n",
    "- Practice implementing RL solutions\n",
    "\n",
    "## Topics Covered\n",
    "1. RL Basics\n",
    "2. Markov Decision Processes\n",
    "3. Value Functions\n",
    "4. Basic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def rl_concepts_example():\n",
    "    # Create simple environment\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    \n",
    "    # Show environment info\n",
    "    print(\"Action Space:\", env.action_space)\n",
    "    print(\"State Space:\", env.observation_space)\n",
    "    \n",
    "    # Run one episode\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "    \n",
    "    print(\"\\nEpisode finished with reward:\", total_reward)\n",
    "\n",
    "rl_concepts_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def q_learning_example():\n",
    "    # Initialize environment\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    \n",
    "    # Q-learning parameters\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 0.99\n",
    "    epsilon = 0.1\n",
    "    episodes = 1000\n",
    "    \n",
    "    # Initialize Q-table\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Training loop\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Update Q-value\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += learning_rate * td_error\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(pd.Series(rewards).rolling(100).mean())\n",
    "    plt.title('Average Reward over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.show()\n",
    "    \n",
    "    return Q\n",
    "\n",
    "Q = q_learning_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def policy_evaluation_example():\n",
    "    # Create simple grid world\n",
    "    grid_size = 4\n",
    "    states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
    "    actions = ['up', 'right', 'down', 'left']\n",
    "    \n",
    "    # Random policy\n",
    "    policy = {state: {action: 0.25 for action in actions} for state in states}\n",
    "    \n",
    "    # Value function\n",
    "    V = {state: 0 for state in states}\n",
    "    \n",
    "    # Reward function\n",
    "    R = {state: -1 for state in states}\n",
    "    R[(grid_size-1, grid_size-1)] = 0  # Goal state\n",
    "    \n",
    "    # Policy evaluation\n",
    "    theta = 0.0001\n",
    "    gamma = 0.9\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            if state == (grid_size-1, grid_size-1):\n",
    "                continue\n",
    "                \n",
    "            v = V[state]\n",
    "            new_v = 0\n",
    "            \n",
    "            for action in actions:\n",
    "                # Get next state\n",
    "                i, j = state\n",
    "                if action == 'up':\n",
    "                    next_state = (max(0, i-1), j)\n",
    "                elif action == 'right':\n",
    "                    next_state = (i, min(grid_size-1, j+1))\n",
    "                elif action == 'down':\n",
    "                    next_state = (min(grid_size-1, i+1), j)\n",
    "                else:  # left\n",
    "                    next_state = (i, max(0, j-1))\n",
    "                \n",
    "                new_v += policy[state][action] * (R[state] + gamma * V[next_state])\n",
    "            \n",
    "            V[state] = new_v\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Visualize value function\n",
    "    value_grid = np.zeros((grid_size, grid_size))\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            value_grid[i, j] = V[(i, j)]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(value_grid)\n",
    "    plt.colorbar()\n",
    "    plt.title('State Value Function')\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            plt.text(j, i, f'{value_grid[i,j]:.2f}',\n",
    "                     ha='center', va='center')\n",
    "    plt.show()\n",
    "\n",
    "policy_evaluation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1: Simple RL Agent\n",
    "\n",
    "def rl_agent_exercise():\n",
    "    print(\"Task: Implement a basic RL agent\")\n",
    "    print(\"1. Create environment\")\n",
    "    print(\"2. Define agent behavior\")\n",
    "    print(\"3. Train agent\")\n",
    "    print(\"4. Evaluate performance\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "rl_agent_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2: Value Function Implementation\n",
    "\n",
    "def value_function_exercise():\n",
    "    print(\"Task: Implement value function estimation\")\n",
    "    print(\"1. Define state space\")\n",
    "    print(\"2. Implement value updates\")\n",
    "    print(\"3. Run iterations\")\n",
    "    print(\"4. Visualize results\")\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "value_function_exercise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ Quiz\n",
    "\n",
    "1. What is reinforcement learning?\n",
    "   - a) Supervised learning\n",
    "   - b) Learning from interaction\n",
    "   - c) Unsupervised learning\n",
    "   - d) Transfer learning\n",
    "\n",
    "2. What is a state in RL?\n",
    "   - a) Action\n",
    "   - b) Environment description\n",
    "   - c) Reward\n",
    "   - d) Policy\n",
    "\n",
    "3. What is a policy?\n",
    "   - a) Reward function\n",
    "   - b) Action selection strategy\n",
    "   - c) State space\n",
    "   - d) Value function\n",
    "\n",
    "4. What is Q-learning?\n",
    "   - a) Policy evaluation\n",
    "   - b) Value function learning\n",
    "   - c) State estimation\n",
    "   - d) Reward calculation\n",
    "\n",
    "5. What is exploration vs exploitation?\n",
    "   - a) Learning rate\n",
    "   - b) Action selection tradeoff\n",
    "   - c) State transition\n",
    "   - d) Reward function\n",
    "\n",
    "6. What is a value function?\n",
    "   - a) Action selection\n",
    "   - b) State/action evaluation\n",
    "   - c) Policy definition\n",
    "   - d) Reward calculation\n",
    "\n",
    "7. What is temporal difference learning?\n",
    "   - a) Policy evaluation\n",
    "   - b) Incremental learning\n",
    "   - c) State estimation\n",
    "   - d) Action selection\n",
    "\n",
    "8. What is the discount factor?\n",
    "   - a) Learning rate\n",
    "   - b) Future reward weight\n",
    "   - c) State value\n",
    "   - d) Action probability\n",
    "\n",
    "9. What is an episode?\n",
    "   - a) State transition\n",
    "   - b) Complete interaction sequence\n",
    "   - c) Reward calculation\n",
    "   - d) Policy update\n",
    "\n",
    "10. What is the Bellman equation?\n",
    "    - a) Policy definition\n",
    "    - b) Value recursion\n",
    "    - c) Action selection\n",
    "    - d) State transition\n",
    "\n",
    "Answers: 1-b, 2-b, 3-b, 4-b, 5-b, 6-b, 7-b, 8-b, 9-b, 10-b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}